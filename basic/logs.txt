* 
* ==> Audit <==
* |-----------|--------------------------------|----------|-------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  | User  | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|-------|---------|---------------------|---------------------|
| start     | --network-plugin=cni           | minikube | qubit | v1.31.1 | 24 Jul 23 16:30 UTC |                     |
|           | --cni=bridge                   |          |       |         |                     |                     |
|           | --container-runtime=containerd |          |       |         |                     |                     |
|           | --bootstrapper=kubeadm         |          |       |         |                     |                     |
| start     |                                | minikube | qubit | v1.31.1 | 24 Jul 23 16:37 UTC |                     |
| start     |                                | minikube | qubit | v1.31.1 | 24 Jul 23 16:39 UTC |                     |
| start     | --network-plugin=cni           | minikube | qubit | v1.31.1 | 24 Jul 23 16:39 UTC |                     |
|           | --cni=bridge                   |          |       |         |                     |                     |
|           | --container-runtime=containerd |          |       |         |                     |                     |
|           | --bootstrapper=kubeadm         |          |       |         |                     |                     |
| start     | --network-plugin=cni           | minikube | qubit | v1.31.1 | 24 Jul 23 16:39 UTC | 24 Jul 23 16:44 UTC |
|           | --cni=bridge                   |          |       |         |                     |                     |
|           | --container-runtime=containerd |          |       |         |                     |                     |
|           | --bootstrapper=kubeadm         |          |       |         |                     |                     |
| start     | --memory 3924                  | minikube | qubit | v1.31.1 | 24 Jul 23 16:45 UTC | 24 Jul 23 16:46 UTC |
| start     | --memory 3924                  | minikube | qubit | v1.31.1 | 24 Jul 23 16:52 UTC |                     |
| stop      |                                | minikube | qubit | v1.31.1 | 24 Jul 23 16:52 UTC | 24 Jul 23 16:52 UTC |
| start     | --memory 3924                  | minikube | qubit | v1.31.1 | 24 Jul 23 16:52 UTC | 24 Jul 23 16:52 UTC |
| addons    | list                           | minikube | qubit | v1.31.1 | 24 Jul 23 16:58 UTC | 24 Jul 23 16:58 UTC |
| addons    | enable dashboard               | minikube | qubit | v1.31.1 | 24 Jul 23 16:58 UTC | 24 Jul 23 16:58 UTC |
| addons    | enable metrics-server          | minikube | qubit | v1.31.1 | 24 Jul 23 16:58 UTC | 24 Jul 23 16:58 UTC |
| dashboard |                                | minikube | qubit | v1.31.1 | 24 Jul 23 16:58 UTC |                     |
| start     | --memory 3920                  | minikube | qubit | v1.31.1 | 24 Jul 23 17:11 UTC | 24 Jul 23 17:11 UTC |
| stop      |                                | minikube | qubit | v1.31.1 | 24 Jul 23 17:19 UTC | 24 Jul 23 17:19 UTC |
| start     | --memory 3920                  | minikube | qubit | v1.31.1 | 24 Jul 23 17:19 UTC | 24 Jul 23 17:19 UTC |
| delete    |                                | minikube | qubit | v1.31.1 | 24 Jul 23 17:21 UTC | 24 Jul 23 17:21 UTC |
| start     | --memory 3920                  | minikube | qubit | v1.31.1 | 24 Jul 23 17:21 UTC |                     |
| start     | --memory 3200                  | minikube | qubit | v1.31.1 | 24 Jul 23 17:22 UTC | 24 Jul 23 17:24 UTC |
| service   | wordpress                      | minikube | qubit | v1.31.1 | 24 Jul 23 17:28 UTC | 24 Jul 23 17:28 UTC |
| start     |                                | minikube | qubit | v1.31.1 | 25 Jul 23 09:39 UTC |                     |
| start     |                                | minikube | qubit | v1.31.1 | 25 Jul 23 09:45 UTC |                     |
| start     | --memory=3200mb                | minikube | qubit | v1.31.1 | 25 Jul 23 09:45 UTC |                     |
| start     | --memory=3200mb                | minikube | qubit | v1.31.1 | 25 Jul 23 09:46 UTC | 25 Jul 23 09:46 UTC |
| start     |                                | minikube | qubit | v1.31.1 | 25 Jul 23 09:59 UTC |                     |
| start     |                                | minikube | qubit | v1.31.1 | 25 Jul 23 10:01 UTC | 25 Jul 23 10:02 UTC |
| service   | wordpress --url                | minikube | qubit | v1.31.1 | 25 Jul 23 10:44 UTC |                     |
| service   | wordpress --url                | minikube | qubit | v1.31.1 | 25 Jul 23 10:44 UTC |                     |
|-----------|--------------------------------|----------|-------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/07/25 10:01:35
Running on machine: qubit
Binary: Built with gc go1.20.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0725 10:01:35.390973 1129968 out.go:296] Setting OutFile to fd 1 ...
I0725 10:01:35.391081 1129968 out.go:348] isatty.IsTerminal(1) = true
I0725 10:01:35.391084 1129968 out.go:309] Setting ErrFile to fd 2...
I0725 10:01:35.391087 1129968 out.go:348] isatty.IsTerminal(2) = true
I0725 10:01:35.391264 1129968 root.go:338] Updating PATH: /home/qubit/.minikube/bin
I0725 10:01:35.556111 1129968 out.go:303] Setting JSON to false
I0725 10:01:35.557341 1129968 start.go:128] hostinfo: {"hostname":"qubit","uptime":411629,"bootTime":1689867666,"procs":331,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.4.0-81-generic","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"6962f986-a07a-4b68-8c02-6d79043443ce"}
I0725 10:01:35.557398 1129968 start.go:138] virtualization:  
I0725 10:01:35.584660 1129968 out.go:177] 😄  minikube v1.31.1 on Ubuntu 20.04
I0725 10:01:35.585922 1129968 notify.go:220] Checking for updates...
I0725 10:01:35.586698 1129968 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0725 10:01:35.586785 1129968 driver.go:373] Setting default libvirt URI to qemu:///system
I0725 10:01:35.837026 1129968 docker.go:121] docker version: linux-20.10.21:
I0725 10:01:35.837115 1129968 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0725 10:01:35.867074 1129968 info.go:266] docker info: {ID:BPN2:PIRA:2ED6:TNNV:SXQV:MLHR:7CL4:535E:PGVW:IQC7:4SDA:4DJP Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:32 OomKillDisable:true NGoroutines:39 SystemTime:2023-07-25 10:01:35.858207374 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.4.0-81-generic OperatingSystem:Ubuntu 20.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4127289344 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:qubit Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No swap limit support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0725 10:01:35.867140 1129968 docker.go:294] overlay module found
I0725 10:01:35.873200 1129968 out.go:177] ✨  Using the docker driver based on existing profile
I0725 10:01:35.891501 1129968 start.go:298] selected driver: docker
I0725 10:01:35.891509 1129968 start.go:898] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40 Memory:3200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/qubit:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0725 10:01:35.891603 1129968 start.go:909] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0725 10:01:35.891676 1129968 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0725 10:01:35.925950 1129968 info.go:266] docker info: {ID:BPN2:PIRA:2ED6:TNNV:SXQV:MLHR:7CL4:535E:PGVW:IQC7:4SDA:4DJP Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:false KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:32 OomKillDisable:true NGoroutines:39 SystemTime:2023-07-25 10:01:35.910032249 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.4.0-81-generic OperatingSystem:Ubuntu 20.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4127289344 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:qubit Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No swap limit support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0725 10:01:35.935185 1129968 out.go:177] 
W0725 10:01:35.944782 1129968 out.go:239] 🧯  The requested memory allocation of 3200MiB does not leave room for system overhead (total system memory: 3936MiB). You may face stability issues.
W0725 10:01:35.944827 1129968 out.go:239] 💡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'
I0725 10:01:35.971830 1129968 out.go:177] 
I0725 10:01:35.972559 1129968 cni.go:84] Creating CNI manager for ""
I0725 10:01:35.972572 1129968 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0725 10:01:35.972581 1129968 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40 Memory:3200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/qubit:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0725 10:01:35.973153 1129968 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0725 10:01:35.973998 1129968 cache.go:122] Beginning downloading kic base image for docker with docker
I0725 10:01:35.974395 1129968 out.go:177] 🚜  Pulling base image ...
I0725 10:01:35.975060 1129968 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0725 10:01:35.975096 1129968 preload.go:148] Found local preload: /home/qubit/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4
I0725 10:01:35.975101 1129968 cache.go:57] Caching tarball of preloaded images
I0725 10:01:35.975148 1129968 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40 in local docker daemon
I0725 10:01:35.975184 1129968 preload.go:174] Found /home/qubit/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0725 10:01:35.975189 1129968 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.3 on docker
I0725 10:01:35.975268 1129968 profile.go:148] Saving config to /home/qubit/.minikube/profiles/minikube/config.json ...
I0725 10:01:35.995377 1129968 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40 in local docker daemon, skipping pull
I0725 10:01:35.995395 1129968 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40 exists in daemon, skipping load
I0725 10:01:35.995408 1129968 cache.go:195] Successfully downloaded all kic artifacts
I0725 10:01:35.995438 1129968 start.go:365] acquiring machines lock for minikube: {Name:mk9992101986d61a08e2e4a6cfafe1686e470c31 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0725 10:01:35.995508 1129968 start.go:369] acquired machines lock for "minikube" in 50.913µs
I0725 10:01:35.995522 1129968 start.go:96] Skipping create...Using existing machine configuration
I0725 10:01:35.995530 1129968 fix.go:54] fixHost starting: 
I0725 10:01:35.995745 1129968 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0725 10:01:36.256200 1129968 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W0725 10:01:36.256226 1129968 fix.go:128] unexpected machine state, will restart: <nil>
I0725 10:01:36.260381 1129968 out.go:177] 🏃  Updating the running docker "minikube" container ...
I0725 10:01:36.261089 1129968 machine.go:88] provisioning docker machine ...
I0725 10:01:36.261114 1129968 ubuntu.go:169] provisioning hostname "minikube"
I0725 10:01:36.261171 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:36.281388 1129968 main.go:141] libmachine: Using SSH client type: native
I0725 10:01:36.281764 1129968 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0725 10:01:36.281770 1129968 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0725 10:01:36.413719 1129968 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0725 10:01:36.413794 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:36.436690 1129968 main.go:141] libmachine: Using SSH client type: native
I0725 10:01:36.437019 1129968 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0725 10:01:36.437029 1129968 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0725 10:01:36.558911 1129968 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0725 10:01:36.558925 1129968 ubuntu.go:175] set auth options {CertDir:/home/qubit/.minikube CaCertPath:/home/qubit/.minikube/certs/ca.pem CaPrivateKeyPath:/home/qubit/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/qubit/.minikube/machines/server.pem ServerKeyPath:/home/qubit/.minikube/machines/server-key.pem ClientKeyPath:/home/qubit/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/qubit/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/qubit/.minikube}
I0725 10:01:36.558945 1129968 ubuntu.go:177] setting up certificates
I0725 10:01:36.558956 1129968 provision.go:83] configureAuth start
I0725 10:01:36.559000 1129968 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0725 10:01:36.580427 1129968 provision.go:138] copyHostCerts
I0725 10:01:36.580469 1129968 exec_runner.go:144] found /home/qubit/.minikube/ca.pem, removing ...
I0725 10:01:36.580474 1129968 exec_runner.go:203] rm: /home/qubit/.minikube/ca.pem
I0725 10:01:36.580531 1129968 exec_runner.go:151] cp: /home/qubit/.minikube/certs/ca.pem --> /home/qubit/.minikube/ca.pem (1074 bytes)
I0725 10:01:36.580602 1129968 exec_runner.go:144] found /home/qubit/.minikube/cert.pem, removing ...
I0725 10:01:36.580604 1129968 exec_runner.go:203] rm: /home/qubit/.minikube/cert.pem
I0725 10:01:36.580619 1129968 exec_runner.go:151] cp: /home/qubit/.minikube/certs/cert.pem --> /home/qubit/.minikube/cert.pem (1119 bytes)
I0725 10:01:36.638344 1129968 exec_runner.go:144] found /home/qubit/.minikube/key.pem, removing ...
I0725 10:01:36.638355 1129968 exec_runner.go:203] rm: /home/qubit/.minikube/key.pem
I0725 10:01:36.638391 1129968 exec_runner.go:151] cp: /home/qubit/.minikube/certs/key.pem --> /home/qubit/.minikube/key.pem (1675 bytes)
I0725 10:01:36.639416 1129968 provision.go:112] generating server cert: /home/qubit/.minikube/machines/server.pem ca-key=/home/qubit/.minikube/certs/ca.pem private-key=/home/qubit/.minikube/certs/ca-key.pem org=qubit.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0725 10:01:36.700841 1129968 provision.go:172] copyRemoteCerts
I0725 10:01:36.700897 1129968 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0725 10:01:36.700930 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:36.719869 1129968 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/qubit/.minikube/machines/minikube/id_rsa Username:docker}
I0725 10:01:36.814766 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0725 10:01:36.832113 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0725 10:01:36.850932 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0725 10:01:36.869123 1129968 provision.go:86] duration metric: configureAuth took 310.159128ms
I0725 10:01:36.869140 1129968 ubuntu.go:193] setting minikube options for container-runtime
I0725 10:01:36.869265 1129968 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0725 10:01:36.869304 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:36.892139 1129968 main.go:141] libmachine: Using SSH client type: native
I0725 10:01:36.892461 1129968 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0725 10:01:36.892466 1129968 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0725 10:01:37.019967 1129968 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0725 10:01:37.019977 1129968 ubuntu.go:71] root file system type: overlay
I0725 10:01:37.020080 1129968 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0725 10:01:37.020130 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:37.039470 1129968 main.go:141] libmachine: Using SSH client type: native
I0725 10:01:37.039830 1129968 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0725 10:01:37.039882 1129968 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=127.0.0.1,192.168.49.2"
Environment="HTTP_PROXY=http://172.16.10.20:3128"
Environment="HTTPS_PROXY=http://172.16.10.20:3128"
Environment="NO_PROXY=127.0.0.1"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0725 10:01:37.182212 1129968 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=127.0.0.1,192.168.49.2
Environment=HTTP_PROXY=http://172.16.10.20:3128
Environment=HTTPS_PROXY=http://172.16.10.20:3128
Environment=NO_PROXY=127.0.0.1


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0725 10:01:37.182278 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:37.207172 1129968 main.go:141] libmachine: Using SSH client type: native
I0725 10:01:37.207555 1129968 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0725 10:01:37.207565 1129968 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0725 10:01:48.630461 1129968 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-07-25 09:59:27.891686486 +0000
+++ /lib/systemd/system/docker.service.new	2023-07-25 10:01:37.175686190 +0000
@@ -12,6 +12,7 @@
 Type=notify
 Restart=on-failure
 
+Environment=NO_PROXY=127.0.0.1,192.168.49.2
 Environment=HTTP_PROXY=http://172.16.10.20:3128
 Environment=HTTPS_PROXY=http://172.16.10.20:3128
 Environment=NO_PROXY=127.0.0.1
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0725 10:01:48.630486 1129968 machine.go:91] provisioned docker machine in 12.369384913s
I0725 10:01:48.630495 1129968 start.go:300] post-start starting for "minikube" (driver="docker")
I0725 10:01:48.630505 1129968 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0725 10:01:48.630552 1129968 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0725 10:01:48.630595 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:48.650813 1129968 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/qubit/.minikube/machines/minikube/id_rsa Username:docker}
I0725 10:01:48.751545 1129968 ssh_runner.go:195] Run: cat /etc/os-release
I0725 10:01:48.754145 1129968 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0725 10:01:48.754175 1129968 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0725 10:01:48.754180 1129968 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0725 10:01:48.754185 1129968 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0725 10:01:48.754191 1129968 filesync.go:126] Scanning /home/qubit/.minikube/addons for local assets ...
I0725 10:01:48.754236 1129968 filesync.go:126] Scanning /home/qubit/.minikube/files for local assets ...
I0725 10:01:48.754249 1129968 start.go:303] post-start completed in 123.747257ms
I0725 10:01:48.754292 1129968 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0725 10:01:48.754331 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:48.773574 1129968 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/qubit/.minikube/machines/minikube/id_rsa Username:docker}
I0725 10:01:48.855526 1129968 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0725 10:01:48.859011 1129968 fix.go:56] fixHost completed within 12.863478847s
I0725 10:01:48.859019 1129968 start.go:83] releasing machines lock for "minikube", held for 12.863505942s
I0725 10:01:48.859072 1129968 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0725 10:01:48.931089 1129968 out.go:177] 🌐  Found network options:
I0725 10:01:48.931570 1129968 out.go:177]     ▪ NO_PROXY=127.0.0.1,192.168.49.2
I0725 10:01:48.931941 1129968 out.go:177]     ▪ http_proxy=http://172.16.10.20:3128
I0725 10:01:48.932372 1129968 out.go:177]     ▪ https_proxy=http://172.16.10.20:3128
I0725 10:01:48.932728 1129968 out.go:177]     ▪ no_proxy=127.0.0.1
I0725 10:01:48.933117 1129968 ssh_runner.go:195] Run: cat /version.json
I0725 10:01:48.933151 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:48.933194 1129968 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0725 10:01:48.933244 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:01:48.957050 1129968 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/qubit/.minikube/machines/minikube/id_rsa Username:docker}
I0725 10:01:48.970911 1129968 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/qubit/.minikube/machines/minikube/id_rsa Username:docker}
I0725 10:01:49.046769 1129968 ssh_runner.go:195] Run: systemctl --version
I0725 10:01:50.256306 1129968 ssh_runner.go:235] Completed: systemctl --version: (1.209518952s)
I0725 10:01:50.256366 1129968 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0725 10:01:50.256585 1129968 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.323376627s)
W0725 10:01:50.256601 1129968 start.go:815] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Failed to connect to registry.k8s.io port 443 after 1202 ms: Connection timed out
W0725 10:01:50.256658 1129968 out.go:239] ❗  This container is having trouble accessing https://registry.k8s.io
W0725 10:01:50.256674 1129968 out.go:239] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0725 10:01:50.261093 1129968 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0725 10:01:50.274877 1129968 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0725 10:01:50.274927 1129968 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0725 10:01:50.282421 1129968 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0725 10:01:50.282438 1129968 start.go:466] detecting cgroup driver to use...
I0725 10:01:50.282462 1129968 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0725 10:01:50.282561 1129968 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0725 10:01:50.294850 1129968 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0725 10:01:50.301689 1129968 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0725 10:01:50.308914 1129968 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0725 10:01:50.308950 1129968 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0725 10:01:50.315136 1129968 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0725 10:01:50.321321 1129968 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0725 10:01:50.327449 1129968 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0725 10:01:50.333995 1129968 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0725 10:01:50.340015 1129968 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0725 10:01:50.346106 1129968 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0725 10:01:50.351347 1129968 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0725 10:01:50.357058 1129968 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0725 10:01:50.441769 1129968 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0725 10:01:50.542781 1129968 start.go:466] detecting cgroup driver to use...
I0725 10:01:50.542810 1129968 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0725 10:01:50.542855 1129968 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0725 10:01:50.555119 1129968 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0725 10:01:50.555163 1129968 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0725 10:01:50.567492 1129968 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0725 10:01:50.581690 1129968 ssh_runner.go:195] Run: which cri-dockerd
I0725 10:01:50.584253 1129968 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0725 10:01:50.590014 1129968 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0725 10:01:50.602523 1129968 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0725 10:01:50.695671 1129968 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0725 10:01:50.790930 1129968 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0725 10:01:50.790962 1129968 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0725 10:01:50.847313 1129968 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0725 10:01:50.929041 1129968 ssh_runner.go:195] Run: sudo systemctl restart docker
I0725 10:01:51.506559 1129968 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0725 10:01:51.587397 1129968 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0725 10:01:51.669924 1129968 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0725 10:01:51.752587 1129968 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0725 10:01:51.839260 1129968 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0725 10:01:51.857264 1129968 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0725 10:01:51.957531 1129968 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0725 10:01:52.072206 1129968 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0725 10:01:52.072256 1129968 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0725 10:01:52.075946 1129968 start.go:534] Will wait 60s for crictl version
I0725 10:01:52.075992 1129968 ssh_runner.go:195] Run: which crictl
I0725 10:01:52.080385 1129968 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0725 10:01:52.122697 1129968 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0725 10:01:52.122749 1129968 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0725 10:01:52.839252 1129968 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0725 10:01:52.884540 1129968 out.go:204] 🐳  Preparing Kubernetes v1.27.3 on Docker 24.0.4 ...
I0725 10:01:52.885304 1129968 out.go:177]     ▪ env NO_PROXY=127.0.0.1,192.168.49.2
I0725 10:01:52.885961 1129968 out.go:177]     ▪ env HTTP_PROXY=http://172.16.10.20:3128
I0725 10:01:52.886459 1129968 out.go:177]     ▪ env HTTPS_PROXY=http://172.16.10.20:3128
I0725 10:01:52.886966 1129968 out.go:177]     ▪ env NO_PROXY=127.0.0.1
I0725 10:01:52.887460 1129968 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0725 10:01:53.028283 1129968 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0725 10:01:53.031522 1129968 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0725 10:01:53.031570 1129968 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0725 10:01:53.047960 1129968 docker.go:636] Got preloaded images: -- stdout --
mysql:8.0
fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
wordpress:6.2.1-apache
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
docker.elastic.co/elasticsearch/elasticsearch:7.15.1
docker.elastic.co/kibana/kibana:7.15.1
gcr.io/k8s-minikube/storage-provisioner:v5
wordpress:4.8-apache

-- /stdout --
I0725 10:01:53.047973 1129968 docker.go:566] Images already preloaded, skipping extraction
I0725 10:01:53.048026 1129968 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0725 10:01:53.063191 1129968 docker.go:636] Got preloaded images: -- stdout --
mysql:8.0
fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
wordpress:6.2.1-apache
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
docker.elastic.co/elasticsearch/elasticsearch:7.15.1
docker.elastic.co/kibana/kibana:7.15.1
gcr.io/k8s-minikube/storage-provisioner:v5
wordpress:4.8-apache

-- /stdout --
I0725 10:01:53.063210 1129968 cache_images.go:84] Images are preloaded, skipping loading
I0725 10:01:53.063260 1129968 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0725 10:01:54.046751 1129968 cni.go:84] Creating CNI manager for ""
I0725 10:01:54.046764 1129968 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0725 10:01:54.046781 1129968 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0725 10:01:54.046794 1129968 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0725 10:01:54.046903 1129968 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0725 10:01:54.059822 1129968 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0725 10:01:54.059883 1129968 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.3
I0725 10:01:54.140354 1129968 binaries.go:44] Found k8s binaries, skipping transfer
I0725 10:01:54.140411 1129968 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0725 10:01:54.147094 1129968 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0725 10:01:54.160504 1129968 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0725 10:01:54.173810 1129968 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0725 10:01:54.246810 1129968 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0725 10:01:54.249494 1129968 certs.go:56] Setting up /home/qubit/.minikube/profiles/minikube for IP: 192.168.49.2
I0725 10:01:54.249509 1129968 certs.go:190] acquiring lock for shared ca certs: {Name:mk83b0734d99ec3bb18d69ec9002d654f421af74 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0725 10:01:54.249626 1129968 certs.go:199] skipping minikubeCA CA generation: /home/qubit/.minikube/ca.key
I0725 10:01:54.270890 1129968 certs.go:199] skipping proxyClientCA CA generation: /home/qubit/.minikube/proxy-client-ca.key
I0725 10:01:54.270969 1129968 certs.go:315] skipping minikube-user signed cert generation: /home/qubit/.minikube/profiles/minikube/client.key
I0725 10:01:54.295695 1129968 certs.go:315] skipping minikube signed cert generation: /home/qubit/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0725 10:01:54.295954 1129968 certs.go:315] skipping aggregator signed cert generation: /home/qubit/.minikube/profiles/minikube/proxy-client.key
I0725 10:01:54.296085 1129968 certs.go:437] found cert: /home/qubit/.minikube/certs/home/qubit/.minikube/certs/ca-key.pem (1675 bytes)
I0725 10:01:54.296116 1129968 certs.go:437] found cert: /home/qubit/.minikube/certs/home/qubit/.minikube/certs/ca.pem (1074 bytes)
I0725 10:01:54.296149 1129968 certs.go:437] found cert: /home/qubit/.minikube/certs/home/qubit/.minikube/certs/cert.pem (1119 bytes)
I0725 10:01:54.296174 1129968 certs.go:437] found cert: /home/qubit/.minikube/certs/home/qubit/.minikube/certs/key.pem (1675 bytes)
I0725 10:01:54.308952 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0725 10:01:54.328391 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0725 10:01:54.345500 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0725 10:01:54.371073 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0725 10:01:54.388388 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0725 10:01:54.405611 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0725 10:01:54.426624 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0725 10:01:54.485806 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0725 10:01:54.504998 1129968 ssh_runner.go:362] scp /home/qubit/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0725 10:01:54.554370 1129968 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0725 10:01:54.568480 1129968 ssh_runner.go:195] Run: openssl version
I0725 10:01:54.622284 1129968 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0725 10:01:54.687753 1129968 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0725 10:01:54.690560 1129968 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jul 24 16:44 /usr/share/ca-certificates/minikubeCA.pem
I0725 10:01:54.690606 1129968 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0725 10:01:54.765773 1129968 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0725 10:01:54.772787 1129968 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0725 10:01:54.775147 1129968 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0725 10:01:54.779981 1129968 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0725 10:01:54.784585 1129968 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0725 10:01:54.789098 1129968 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0725 10:01:54.793721 1129968 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0725 10:01:54.798393 1129968 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0725 10:01:54.802683 1129968 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40 Memory:3200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/qubit:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0725 10:01:54.802774 1129968 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0725 10:01:54.816073 1129968 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0725 10:01:54.823004 1129968 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0725 10:01:54.823010 1129968 kubeadm.go:636] restartCluster start
I0725 10:01:54.823047 1129968 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0725 10:01:54.828917 1129968 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0725 10:01:54.829314 1129968 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0725 10:01:54.975282 1129968 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0725 10:01:54.992538 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:54.992576 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:54.999966 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:55.500902 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:55.500960 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:55.509450 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:56.000170 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:56.000228 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:56.008749 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:56.500253 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:56.500328 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:56.508666 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:57.000207 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:57.000268 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:57.009171 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:57.500757 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:57.500831 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:57.510374 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:58.000671 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:58.000733 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:58.009435 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:58.500996 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:58.501056 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:58.510199 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:59.000636 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:59.000709 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:59.023909 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:01:59.500058 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:01:59.500122 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:01:59.510620 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:02:00.000081 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:02:00.000139 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:02:00.009005 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:02:00.500349 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:02:00.597247 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:02:00.605796 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:02:01.000213 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:02:01.000267 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:02:01.019313 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:02:01.500654 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:02:01.500722 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:02:01.509491 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:02:02.000721 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:02:02.000781 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:02:02.010771 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:02:02.500088 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:02:02.500155 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0725 10:02:02.510234 1129968 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0725 10:02:03.000748 1129968 api_server.go:166] Checking apiserver status ...
I0725 10:02:03.000814 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0725 10:02:03.010129 1129968 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/29387/cgroup
I0725 10:02:03.016879 1129968 api_server.go:182] apiserver freezer: "9:freezer:/docker/7ce5b7207713855ddc93bc4dac5c246478f4e40df408c62d6b7d92a80448cbf5/kubepods/burstable/pod4e275e35949ad3fdfeb753c1099308e7/f9b42d9d9f37c61d9fc684cceef5f90cd082ebd37338ae744842a29125cbaafa"
I0725 10:02:03.016923 1129968 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/docker/7ce5b7207713855ddc93bc4dac5c246478f4e40df408c62d6b7d92a80448cbf5/kubepods/burstable/pod4e275e35949ad3fdfeb753c1099308e7/f9b42d9d9f37c61d9fc684cceef5f90cd082ebd37338ae744842a29125cbaafa/freezer.state
I0725 10:02:03.023291 1129968 api_server.go:204] freezer state: "THAWED"
I0725 10:02:03.023303 1129968 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0725 10:02:05.042141 1129968 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0725 10:02:05.042162 1129968 retry.go:31] will retry after 209.520018ms: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0725 10:02:05.252654 1129968 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0725 10:02:05.260116 1129968 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0725 10:02:05.260140 1129968 retry.go:31] will retry after 271.128706ms: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0725 10:02:05.531668 1129968 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0725 10:02:05.535245 1129968 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0725 10:02:05.535264 1129968 retry.go:31] will retry after 304.960778ms: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0725 10:02:05.840641 1129968 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0725 10:02:05.844382 1129968 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0725 10:02:05.844406 1129968 retry.go:31] will retry after 489.704848ms: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0725 10:02:06.335052 1129968 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0725 10:02:06.338972 1129968 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0725 10:02:06.454063 1129968 system_pods.go:86] 8 kube-system pods found
I0725 10:02:06.454081 1129968 system_pods.go:89] "coredns-5d78c9869d-2sjp8" [be6fba94-60a1-4142-b1eb-31163b3beb7a] Running
I0725 10:02:06.454088 1129968 system_pods.go:89] "etcd-minikube" [872fbaae-1e65-4509-8258-dc5590be60fb] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0725 10:02:06.454095 1129968 system_pods.go:89] "fluentd-jxmvh" [a57631ca-5a20-480d-871d-a9fd9bf85657] Running / Ready:ContainersNotReady (containers with unready status: [fluentd]) / ContainersReady:ContainersNotReady (containers with unready status: [fluentd])
I0725 10:02:06.454100 1129968 system_pods.go:89] "kube-apiserver-minikube" [2ea300df-0781-429b-bb5b-000c6e0e2ef1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0725 10:02:06.454107 1129968 system_pods.go:89] "kube-controller-manager-minikube" [7bf6dc51-fd7c-4d17-9be4-7c3d869e281d] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0725 10:02:06.454111 1129968 system_pods.go:89] "kube-proxy-xs8fq" [2c8e03f5-000d-48ea-9287-f5e74f36db11] Running
I0725 10:02:06.454116 1129968 system_pods.go:89] "kube-scheduler-minikube" [2235c06e-2978-4930-9d55-88ee43ddbf51] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0725 10:02:06.454121 1129968 system_pods.go:89] "storage-provisioner" [5d5c9c81-aa0e-4bff-ae8f-24ba044e4dc6] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0725 10:02:06.455371 1129968 api_server.go:141] control plane version: v1.27.3
I0725 10:02:06.455381 1129968 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0725 10:02:06.455385 1129968 kubeadm.go:684] Taking a shortcut, as the cluster seems to be properly configured
I0725 10:02:06.455395 1129968 kubeadm.go:640] restartCluster took 11.632382722s
I0725 10:02:06.455399 1129968 kubeadm.go:406] StartCluster complete in 11.652720927s
I0725 10:02:06.455411 1129968 settings.go:142] acquiring lock: {Name:mkbacc7d31e8f0c9ca75b04112a3847ae24f3c75 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0725 10:02:06.455463 1129968 settings.go:150] Updating kubeconfig:  /home/qubit/.kube/config
I0725 10:02:06.455881 1129968 lock.go:35] WriteFile acquiring /home/qubit/.kube/config: {Name:mkebf6184679fd7091a1764129e7e84ce5c76f97 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0725 10:02:06.456283 1129968 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0725 10:02:06.456476 1129968 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0725 10:02:06.456581 1129968 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0725 10:02:06.456642 1129968 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0725 10:02:06.456658 1129968 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0725 10:02:06.456661 1129968 addons.go:240] addon storage-provisioner should already be in state true
I0725 10:02:06.456684 1129968 host.go:66] Checking if "minikube" exists ...
I0725 10:02:06.456933 1129968 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0725 10:02:06.469707 1129968 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0725 10:02:06.469734 1129968 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0725 10:02:06.469944 1129968 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0725 10:02:06.500991 1129968 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0725 10:02:06.499663 1129968 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0725 10:02:06.511033 1129968 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0725 10:02:06.513773 1129968 out.go:177] 🔎  Verifying Kubernetes components...
I0725 10:02:06.511580 1129968 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0725 10:02:06.543872 1129968 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0725 10:02:06.543941 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:02:06.549906 1129968 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0725 10:02:06.570031 1129968 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0725 10:02:06.570040 1129968 addons.go:240] addon default-storageclass should already be in state true
I0725 10:02:06.570059 1129968 host.go:66] Checking if "minikube" exists ...
I0725 10:02:06.570322 1129968 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0725 10:02:06.589189 1129968 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/qubit/.minikube/machines/minikube/id_rsa Username:docker}
I0725 10:02:06.602438 1129968 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0725 10:02:06.602448 1129968 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0725 10:02:06.602496 1129968 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0725 10:02:06.639002 1129968 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/qubit/.minikube/machines/minikube/id_rsa Username:docker}
I0725 10:02:06.719070 1129968 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0725 10:02:06.765212 1129968 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0725 10:02:07.819905 1129968 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.363598614s)
I0725 10:02:07.819946 1129968 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.270023264s)
I0725 10:02:07.819952 1129968 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0725 10:02:07.819972 1129968 api_server.go:52] waiting for apiserver process to appear ...
I0725 10:02:07.820012 1129968 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0725 10:02:10.228762 1129968 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.50967164s)
I0725 10:02:10.228817 1129968 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.46359562s)
I0725 10:02:10.329917 1129968 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0725 10:02:10.228960 1129968 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.408939618s)
I0725 10:02:10.348876 1129968 addons.go:502] enable addons completed in 3.892299471s: enabled=[storage-provisioner default-storageclass]
I0725 10:02:10.348891 1129968 api_server.go:72] duration metric: took 3.837827571s to wait for apiserver process to appear ...
I0725 10:02:10.348898 1129968 api_server.go:88] waiting for apiserver healthz status ...
I0725 10:02:10.348909 1129968 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0725 10:02:10.355822 1129968 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0725 10:02:10.356529 1129968 api_server.go:141] control plane version: v1.27.3
I0725 10:02:10.356537 1129968 api_server.go:131] duration metric: took 7.635739ms to wait for apiserver health ...
I0725 10:02:10.356551 1129968 system_pods.go:43] waiting for kube-system pods to appear ...
I0725 10:02:10.360783 1129968 system_pods.go:59] 8 kube-system pods found
I0725 10:02:10.360791 1129968 system_pods.go:61] "coredns-5d78c9869d-2sjp8" [be6fba94-60a1-4142-b1eb-31163b3beb7a] Running
I0725 10:02:10.360795 1129968 system_pods.go:61] "etcd-minikube" [872fbaae-1e65-4509-8258-dc5590be60fb] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0725 10:02:10.360801 1129968 system_pods.go:61] "fluentd-jxmvh" [a57631ca-5a20-480d-871d-a9fd9bf85657] Running / Ready:ContainersNotReady (containers with unready status: [fluentd]) / ContainersReady:ContainersNotReady (containers with unready status: [fluentd])
I0725 10:02:10.360806 1129968 system_pods.go:61] "kube-apiserver-minikube" [2ea300df-0781-429b-bb5b-000c6e0e2ef1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0725 10:02:10.360810 1129968 system_pods.go:61] "kube-controller-manager-minikube" [7bf6dc51-fd7c-4d17-9be4-7c3d869e281d] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0725 10:02:10.360812 1129968 system_pods.go:61] "kube-proxy-xs8fq" [2c8e03f5-000d-48ea-9287-f5e74f36db11] Running
I0725 10:02:10.360815 1129968 system_pods.go:61] "kube-scheduler-minikube" [2235c06e-2978-4930-9d55-88ee43ddbf51] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0725 10:02:10.360819 1129968 system_pods.go:61] "storage-provisioner" [5d5c9c81-aa0e-4bff-ae8f-24ba044e4dc6] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0725 10:02:10.360822 1129968 system_pods.go:74] duration metric: took 4.268661ms to wait for pod list to return data ...
I0725 10:02:10.360828 1129968 kubeadm.go:581] duration metric: took 3.849767588s to wait for : map[apiserver:true system_pods:true] ...
I0725 10:02:10.360839 1129968 node_conditions.go:102] verifying NodePressure condition ...
I0725 10:02:10.363267 1129968 node_conditions.go:122] node storage ephemeral capacity is 20511312Ki
I0725 10:02:10.363279 1129968 node_conditions.go:123] node cpu capacity is 2
I0725 10:02:10.363286 1129968 node_conditions.go:105] duration metric: took 2.445115ms to run NodePressure ...
I0725 10:02:10.363293 1129968 start.go:228] waiting for startup goroutines ...
I0725 10:02:10.363297 1129968 start.go:233] waiting for cluster config update ...
I0725 10:02:10.363304 1129968 start.go:242] writing updated cluster config ...
I0725 10:02:10.363663 1129968 ssh_runner.go:195] Run: rm -f paused
I0725 10:02:10.559281 1129968 start.go:596] kubectl: 1.27.4, cluster: 1.27.3 (minor skew: 0)
I0725 10:02:10.979023 1129968 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jul 25 10:02:02 minikube cri-dockerd[27989]: time="2023-07-25T10:02:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/13be0228e8466164136995c260e82903f904d35ef74102052c155d7f5c96be22/resolv.conf as [nameserver 10.96.0.10 search logging.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:02:02 minikube cri-dockerd[27989]: time="2023-07-25T10:02:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/feae9578db25f69465945c9e12b6dff296e3d0f03672fa395a249eac37e6876b/resolv.conf as [nameserver 10.96.0.10 search logging.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:02:02 minikube cri-dockerd[27989]: time="2023-07-25T10:02:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/75ded591914fc6f1dd83bb9cb59085b4e252c7c9a6286102f00d652c24f575e2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:02:02 minikube dockerd[27758]: time="2023-07-25T10:02:02.710445575Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Jul 25 10:02:02 minikube dockerd[27758]: time="2023-07-25T10:02:02.715463681Z" level=warning msg="Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap."
Jul 25 10:03:43 minikube dockerd[27758]: time="2023-07-25T10:03:43.904309814Z" level=info msg="ignoring event" container=f19e3a7800d855f6a73722f565c097381ec26de14940fcf2e966addd577b1101 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:06:55 minikube dockerd[27758]: time="2023-07-25T10:06:55.054553535Z" level=info msg="ignoring event" container=cb063e3c2c4621b2d069391c6df8194a506a23df2b8433e641b55fc076b578f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:09:33 minikube dockerd[27758]: time="2023-07-25T10:09:33.921744813Z" level=info msg="ignoring event" container=1204bdcc141e5b13d251e316b400ac43e89cc74d85c9cfcafc85af478d7958b8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:12:08 minikube dockerd[27758]: time="2023-07-25T10:12:08.073992476Z" level=info msg="ignoring event" container=81f8089f834864da559a3680f4d1a55ff6cf51a23a7837ca927ba1298b7783a4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:12:34 minikube cri-dockerd[27989]: time="2023-07-25T10:12:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/11816442673f6cca32ec06f164e6032865888dfd985876e418a852ba77cf99d8/resolv.conf as [nameserver 10.96.0.10 search logging.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:12:40 minikube dockerd[27758]: time="2023-07-25T10:12:40.464002424Z" level=info msg="ignoring event" container=8f720308e70aaafc4d0ecd16a6345ec77c41253671a20bb8311296e04379173f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:12:44 minikube dockerd[27758]: time="2023-07-25T10:12:44.698130979Z" level=info msg="ignoring event" container=385284f953b17fbae78b309a4991f5ba7f0077212eb903080bc7ca99baa31ba8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:13:02 minikube dockerd[27758]: time="2023-07-25T10:13:02.180426174Z" level=info msg="ignoring event" container=b15f11ab0c8a124dbfb0683143499a446b4cdc79cdfe9c6fa33cc8fcea9c5335 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:13:31 minikube dockerd[27758]: time="2023-07-25T10:13:31.811374857Z" level=info msg="ignoring event" container=eaca2bf3ed59ba97ff3045d18d7e0009862ad855fcc0714da3599631f9674ca3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:14:24 minikube dockerd[27758]: time="2023-07-25T10:14:24.766192600Z" level=info msg="ignoring event" container=b8b929b204f71cc1d98e2d263bb47940e094c018438c4fc63bda3a1fc7c717ab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:15:05 minikube dockerd[27758]: time="2023-07-25T10:15:05.299964552Z" level=info msg="ignoring event" container=2c951b623e0d545e29b29157b03ec83515c22243fd6185f794b40c8080d85a64 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:15:54 minikube dockerd[27758]: time="2023-07-25T10:15:54.442482791Z" level=info msg="ignoring event" container=bcda1fd4b6e8553ae95b16cb533f735661d7d407d34278e85ba71d9de45d2da3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:17:14 minikube dockerd[27758]: time="2023-07-25T10:17:14.928232293Z" level=info msg="ignoring event" container=ecf49dddad9114f32714566ae511f651306983228ae77aa1680686f66ecd75bf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:18:42 minikube dockerd[27758]: time="2023-07-25T10:18:42.596608730Z" level=info msg="ignoring event" container=703e8905ea12d433f6b1d4c47727a51e48400babe9c99ad56488cfe32716366f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:20:38 minikube dockerd[27758]: time="2023-07-25T10:20:38.583455881Z" level=info msg="ignoring event" container=b9ad290d6bc22f6f3c68ac50293074905d75cb13b90257f7acbb306f24c0a8a8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:22:24 minikube dockerd[27758]: time="2023-07-25T10:22:24.765461372Z" level=info msg="ignoring event" container=dc7e036dcb071951403712d68ad418b8dac602f01ac45648cf6ab2d4583aa757 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:23:39 minikube dockerd[27758]: time="2023-07-25T10:23:39.550018589Z" level=info msg="ignoring event" container=4c43dac1886ae028484cb036b73050c640b995bf61d34263c9f1cdf6218e107f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:23:45 minikube cri-dockerd[27989]: time="2023-07-25T10:23:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0a5f0331c552bd2926d4dd5b24b7f057760425bb9c307fea207bb26b9b5c2f8c/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:23:47 minikube dockerd[27758]: time="2023-07-25T10:23:47.897729118Z" level=info msg="ignoring event" container=5a8cea4ce99201b098268bf6e66eebd838bc7de58528f09b608d9db2f92d80da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:23:47 minikube dockerd[27758]: time="2023-07-25T10:23:47.899786542Z" level=info msg="ignoring event" container=111180c8bc1c1969fbd44df31d2d9cf308d39fe7b39f7497f7dea8a925e771ad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:23:49 minikube dockerd[27758]: time="2023-07-25T10:23:49.838250739Z" level=info msg="ignoring event" container=75a15970ef5136a063c88ed525e53ec848709cdbf1d865ddb7dc6657eeddff09 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:24:05 minikube dockerd[27758]: time="2023-07-25T10:24:05.580306864Z" level=info msg="ignoring event" container=7c515c96fa697993b58c02d5936fe415548a2ce5cda880ac4270b66a49e45254 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:24:10 minikube dockerd[27758]: time="2023-07-25T10:24:10.428717719Z" level=info msg="ignoring event" container=0a5f0331c552bd2926d4dd5b24b7f057760425bb9c307fea207bb26b9b5c2f8c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:24:12 minikube cri-dockerd[27989]: time="2023-07-25T10:24:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/51487b2024da616ed7dbd81674ec646b56649c6ba7fe85a6bbadb737c3f706a9/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:24:13 minikube dockerd[27758]: time="2023-07-25T10:24:13.842085478Z" level=info msg="ignoring event" container=1dffd2045107528204c064c54af0c867f268cdf1f8719afe7869167df873192d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:24:14 minikube dockerd[27758]: time="2023-07-25T10:24:14.625850442Z" level=info msg="ignoring event" container=2bc8c5d14d20855bc5ea634bddffadfc18e755083036dafdfb9dbb03f29c5bb6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:24:29 minikube dockerd[27758]: time="2023-07-25T10:24:29.358096347Z" level=info msg="ignoring event" container=655fb49fe1e50e4fa54a54e53d1a8623439550b143fdaa0eac2dba94c68a8040 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:24:53 minikube dockerd[27758]: time="2023-07-25T10:24:53.756014194Z" level=info msg="ignoring event" container=ef2f4ed922648410978f453f4a9ffb9d674a322eb13e9ce2014a51667075de67 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:25:44 minikube dockerd[27758]: time="2023-07-25T10:25:44.597930821Z" level=info msg="ignoring event" container=c8739b602c2b7af8439c1df137a5df8dd9022a4473aa7cb7015e7f7d01e4042a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:25:47 minikube cri-dockerd[27989]: time="2023-07-25T10:25:47Z" level=error msg="Error response from daemon: No such container: ef2f4ed922648410978f453f4a9ffb9d674a322eb13e9ce2014a51667075de67 Failed to get stats from container ef2f4ed922648410978f453f4a9ffb9d674a322eb13e9ce2014a51667075de67"
Jul 25 10:26:14 minikube dockerd[27758]: time="2023-07-25T10:26:14.193897341Z" level=info msg="ignoring event" container=7d80deff58b4017f7aead7d86f1582c1a26f3d8e1e2b5e74062213aaa45a4b74 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:27:09 minikube dockerd[27758]: time="2023-07-25T10:27:09.015526707Z" level=info msg="ignoring event" container=c0827dac75c55e4d7280dc2c61f6ed8b392ed0b5157b4857d1982e24e770053a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:28:58 minikube dockerd[27758]: time="2023-07-25T10:28:58.970762358Z" level=info msg="ignoring event" container=3f3f10a8af36af8162ec7544256e1ab221dbb905f21976e3a7fd1a5031dff410 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:30:04 minikube dockerd[27758]: time="2023-07-25T10:30:04.232064702Z" level=info msg="ignoring event" container=ffc21e97fdeb8282cdc060c6b8e4f837235e5f91ba93432e036603d4e4cc559d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:30:12 minikube dockerd[27758]: time="2023-07-25T10:30:12.113880388Z" level=info msg="ignoring event" container=51487b2024da616ed7dbd81674ec646b56649c6ba7fe85a6bbadb737c3f706a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:30:14 minikube cri-dockerd[27989]: time="2023-07-25T10:30:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb39ad70d1ad56f89392be2563e53989c32cbaf230942ac450d4938963542116/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:30:15 minikube dockerd[27758]: time="2023-07-25T10:30:15.083368122Z" level=info msg="ignoring event" container=f485f4ae8324787d41331c2bdd0874617f121703eabcabd7b39ff192e374a114 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:30:16 minikube dockerd[27758]: time="2023-07-25T10:30:16.387722428Z" level=info msg="ignoring event" container=f7a9edb34b1057bab79fb7de48e6f3dcbf9e325c89444faf5cc267169873a2be module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:30:30 minikube dockerd[27758]: time="2023-07-25T10:30:30.435489705Z" level=info msg="ignoring event" container=655a67afbc5db778da6f1f19accffb30752ef4e89cb510e46d1db4f651b04911 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:30:59 minikube dockerd[27758]: time="2023-07-25T10:30:59.522716104Z" level=info msg="ignoring event" container=668e3d0e58f549f0370c4d693164858b6870105f25dcf06a0e3b61ae3714a7f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:31:54 minikube dockerd[27758]: time="2023-07-25T10:31:54.190349727Z" level=info msg="ignoring event" container=18175d68aad109c8e6cb42365ba2441b593b32b26003d92757ae078ceae24355 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:31:56 minikube dockerd[27758]: time="2023-07-25T10:31:56.177912733Z" level=info msg="ignoring event" container=04a0c24ba9949cc457252bda46430194c581d4cc361237edec5d8de7eedbf031 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:33:24 minikube dockerd[27758]: time="2023-07-25T10:33:24.404830398Z" level=info msg="ignoring event" container=6b99ea4517abbf7a0b63d4a46a95c753666cd5cf84b5a607506087604f7c4a42 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:34:11 minikube dockerd[27758]: time="2023-07-25T10:34:11.641311806Z" level=info msg="ignoring event" container=efd12e66284746086fb6e46c73b206aa5468b2004c354b8cf29ace0418ac514e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:36:16 minikube dockerd[27758]: time="2023-07-25T10:36:16.336981648Z" level=info msg="ignoring event" container=efc64014a23dc476dd16237551f043f2f5093cada26a9de8456908acbfa3dc3a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:37:27 minikube dockerd[27758]: time="2023-07-25T10:37:27.228476425Z" level=info msg="ignoring event" container=a9a20a98795d81415daaa61d580477e6ccd1889717a9f5ece3364bfefbfa3ddd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:39:19 minikube dockerd[27758]: time="2023-07-25T10:39:19.390060960Z" level=info msg="ignoring event" container=70328b72d256c73244173872677389661ede4592365adb2896db03b680063bee module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:41:25 minikube dockerd[27758]: time="2023-07-25T10:41:25.617797003Z" level=info msg="ignoring event" container=64c2a72c2f249a2741093315993324f17e65852aea806da72505e3b9bc42ba43 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:43:04 minikube dockerd[27758]: time="2023-07-25T10:43:04.759139919Z" level=info msg="ignoring event" container=30482cbd5feeee670351a782128b3fd3d67357fc4ff2fe4c60f182bb97d384fc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:43:40 minikube dockerd[27758]: time="2023-07-25T10:43:40.359792145Z" level=info msg="ignoring event" container=071ad77f8fb24216a999c90f6ca22123c7d295086691f243483e043fd861f265 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:43:47 minikube cri-dockerd[27989]: time="2023-07-25T10:43:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/65cab73fb1f7481527d1696b8eb47b2173133f30910983d93e413025c9110a50/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:43:51 minikube dockerd[27758]: time="2023-07-25T10:43:51.185992165Z" level=info msg="ignoring event" container=203c4e3dc14c282281a8f836d9657411a431cea836ff2386e65b736a71b1cb94 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:43:52 minikube dockerd[27758]: time="2023-07-25T10:43:52.797671582Z" level=info msg="ignoring event" container=75ded591914fc6f1dd83bb9cb59085b4e252c7c9a6286102f00d652c24f575e2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 25 10:43:55 minikube cri-dockerd[27989]: time="2023-07-25T10:43:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d310cd74f6c87c06c53b69b704df40c23319484f616bec2d83d16b818b41e136/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 25 10:44:24 minikube dockerd[27758]: time="2023-07-25T10:44:24.088607255Z" level=info msg="ignoring event" container=66749532e8f1a5bf18cb9e1e1966d7b51d808d5b1b59ed3f647469bdec547e48 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
66749532e8f1a       4654ba3359c97       34 seconds ago      Exited              fluentd                   11                  11816442673f6       fluentd-kwbpj
64c2a72c2f249       4654ba3359c97       3 minutes ago       Exited              fluentd                   7                   bb39ad70d1ad5       fluentd-rhj8j
17f784edca0b4       6e38f40d628db       41 minutes ago      Running             storage-provisioner       7                   a96c4da30d748       storage-provisioner
a51585fe34e99       fa601f7c24cb4       42 minutes ago      Running             elasticsearch             3                   feae9578db25f       elasticsearch-67468595b7-j9mzq
a5b9ec0c77994       9871707dda25a       42 minutes ago      Running             kibana                    3                   13be0228e8466       kibana-79c4f7d4c8-k7xpv
2632a13964958       7cffc01dba0e1       42 minutes ago      Running             kube-controller-manager   5                   e6ed810a90f20       kube-controller-manager-minikube
c86bef1f5aa14       41697ceeb70b3       42 minutes ago      Running             kube-scheduler            4                   db46c48f65b5e       kube-scheduler-minikube
f9b42d9d9f37c       08a0c939e61b7       42 minutes ago      Running             kube-apiserver            4                   b4a092873ceb7       kube-apiserver-minikube
b46ae6900d5a8       ead0a4a53df89       42 minutes ago      Running             coredns                   3                   ef278d48399e6       coredns-5d78c9869d-2sjp8
af629843e3ef7       86b6af7dd652c       42 minutes ago      Running             etcd                      4                   424c31940e38c       etcd-minikube
2f0560665690e       5780543258cf0       42 minutes ago      Running             kube-proxy                3                   6cfe9019c8115       kube-proxy-xs8fq
9d2f13e96235d       6e38f40d628db       42 minutes ago      Exited              storage-provisioner       6                   a96c4da30d748       storage-provisioner
acc34c3aa81a8       7cffc01dba0e1       43 minutes ago      Exited              kube-controller-manager   4                   9229c01e8fdf5       kube-controller-manager-minikube
0a4109f184686       ead0a4a53df89       44 minutes ago      Exited              coredns                   2                   c14e7132f37ff       coredns-5d78c9869d-2sjp8
4548e9df158ff       08a0c939e61b7       44 minutes ago      Exited              kube-apiserver            3                   0e776f0033458       kube-apiserver-minikube
86485437ecbd5       5780543258cf0       44 minutes ago      Exited              kube-proxy                2                   1f4dfe66bbafa       kube-proxy-xs8fq
b1e59d90d7973       fa601f7c24cb4       44 minutes ago      Exited              elasticsearch             2                   547f479cd4845       elasticsearch-67468595b7-j9mzq
f0f6377ee4237       9871707dda25a       44 minutes ago      Exited              kibana                    2                   58e1dedd449c3       kibana-79c4f7d4c8-k7xpv
601aa25927b8c       86b6af7dd652c       44 minutes ago      Exited              etcd                      3                   b7b4505176294       etcd-minikube
5504c63ee3cec       41697ceeb70b3       44 minutes ago      Exited              kube-scheduler            3                   ae064236d711b       kube-scheduler-minikube

* 
* ==> coredns [0a4109f18468] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:44467 - 24651 "HINFO IN 1544649244096103227.2214063548336550064. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.005860927s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [b46ae6900d5a] <==
* [INFO] 10.244.0.24:50920 - 57942 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000179124s
[INFO] 10.244.0.24:50920 - 19448 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000064424s
[INFO] 10.244.0.24:52467 - 49879 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000087168s
[INFO] 10.244.0.24:52467 - 7110 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000074137s
[INFO] 10.244.0.24:53155 - 32201 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000101131s
[INFO] 10.244.0.24:53155 - 10302 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000140274s
[INFO] 10.244.0.24:60265 - 8919 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000067329s
[INFO] 10.244.0.24:60265 - 31231 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000074366s
[INFO] 10.244.0.24:57973 - 27510 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000092052s
[INFO] 10.244.0.24:57973 - 44720 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000046477s
[INFO] 10.244.0.24:39398 - 21468 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000116892s
[INFO] 10.244.0.24:39398 - 59040 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000252812s
[INFO] 10.244.0.24:41232 - 61038 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000075733s
[INFO] 10.244.0.24:41232 - 14097 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000040247s
[INFO] 10.244.0.24:47044 - 44541 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000126901s
[INFO] 10.244.0.24:47044 - 29312 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000231115s
[INFO] 10.244.0.24:36262 - 27197 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000071567s
[INFO] 10.244.0.24:36262 - 21899 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000025706s
[INFO] 10.244.0.24:60746 - 46606 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000078262s
[INFO] 10.244.0.24:60746 - 27044 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000081298s
[INFO] 10.244.0.24:41933 - 65348 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000207977s
[INFO] 10.244.0.24:41933 - 6849 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000460391s
[INFO] 10.244.0.24:34923 - 33756 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000096369s
[INFO] 10.244.0.24:34923 - 4616 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000038759s
[INFO] 10.244.0.24:39190 - 28620 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000101687s
[INFO] 10.244.0.24:39190 - 4274 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000062046s
[INFO] 10.244.0.24:40445 - 23895 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000067802s
[INFO] 10.244.0.24:40445 - 27144 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000067278s
[INFO] 10.244.0.24:44427 - 38748 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000074016s
[INFO] 10.244.0.24:44427 - 12994 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000130524s
[INFO] 10.244.0.24:49516 - 459 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000068658s
[INFO] 10.244.0.24:49516 - 18151 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000079123s
[INFO] 10.244.0.24:49852 - 29461 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000075614s
[INFO] 10.244.0.24:49852 - 41043 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000076071s
[INFO] 10.244.0.24:46677 - 6551 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000075192s
[INFO] 10.244.0.24:46677 - 49907 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000114344s
[INFO] 10.244.0.24:42695 - 35170 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000111376s
[INFO] 10.244.0.24:42695 - 30436 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000071476s
[INFO] 10.244.0.24:48773 - 63178 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000144373s
[INFO] 10.244.0.24:48773 - 19921 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000089939s
[INFO] 10.244.0.24:51271 - 36328 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000195881s
[INFO] 10.244.0.24:51271 - 26786 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000247978s
[INFO] 10.244.0.24:45529 - 36084 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000086804s
[INFO] 10.244.0.24:45529 - 47886 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000035887s
[INFO] 10.244.0.24:44082 - 574 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000091963s
[INFO] 10.244.0.24:44082 - 63355 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000070631s
[INFO] 10.244.0.24:51946 - 13924 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000086308s
[INFO] 10.244.0.24:51946 - 26974 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000032858s
[INFO] 10.244.0.24:36569 - 64477 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000080147s
[INFO] 10.244.0.24:36569 - 32276 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.00002599s
[INFO] 10.244.0.24:33032 - 44991 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000089616s
[INFO] 10.244.0.24:33032 - 49861 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000157456s
[INFO] 10.244.0.24:39615 - 28783 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000081485s
[INFO] 10.244.0.24:39615 - 6969 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000024547s
[INFO] 10.244.0.24:41156 - 64651 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000076533s
[INFO] 10.244.0.24:41156 - 18415 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000051013s
[INFO] 10.244.0.24:59011 - 25451 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000091435s
[INFO] 10.244.0.24:59011 - 20134 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000150746s
[INFO] 10.244.0.24:49700 - 60510 "A IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000082618s
[INFO] 10.244.0.24:49700 - 16224 "AAAA IN wordpress-mysql.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000036118s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd3f3801765d093a485d255043149f92ec0a695f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_07_24T17_24_36_0700
                    minikube.k8s.io/version=v1.31.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 24 Jul 2023 17:24:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 25 Jul 2023 10:44:49 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 25 Jul 2023 10:44:42 +0000   Mon, 24 Jul 2023 17:24:33 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 25 Jul 2023 10:44:42 +0000   Mon, 24 Jul 2023 17:24:33 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 25 Jul 2023 10:44:42 +0000   Mon, 24 Jul 2023 17:24:33 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 25 Jul 2023 10:44:42 +0000   Tue, 25 Jul 2023 09:48:39 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  20511312Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4030556Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  20511312Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4030556Ki
  pods:               110
System Info:
  Machine ID:                 8324b8c68c1d4d52a91eca8374617b30
  System UUID:                df0e6a95-6d89-4aee-aea2-07448c269fbc
  Boot ID:                    ad52be56-4f3d-4236-b7fc-bfaa2a5ea328
  Kernel Version:             5.4.0-81-generic
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.3
  Kube-Proxy Version:         v1.27.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     wordpress-78bb764d54-8llzf          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74s
  default                     wordpress-mysql-b759dbb45-2b5h7     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         63s
  kube-system                 coredns-5d78c9869d-2sjp8            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     17h
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         17h
  kube-system                 fluentd-rhj8j                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14m
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17h
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17h
  kube-system                 kube-proxy-xs8fq                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17h
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17h
  logging                     elasticsearch-67468595b7-j9mzq      0 (0%!)(MISSING)        0 (0%!)(MISSING)      2Gi (52%!)(MISSING)        2Gi (52%!)(MISSING)      17h
  logging                     fluentd-kwbpj                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32m
  logging                     kibana-79c4f7d4c8-k7xpv             0 (0%!)(MISSING)        0 (0%!)(MISSING)      1Gi (26%!)(MISSING)        1Gi (26%!)(MISSING)      17h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                750m (37%!)(MISSING)    0 (0%!)(MISSING)
  memory             3242Mi (82%!)(MISSING)  3242Mi (82%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 57m                kube-proxy       
  Normal   Starting                 43m                kube-proxy       
  Normal   Starting                 42m                kube-proxy       
  Normal   Starting                 58m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientPID     58m (x7 over 58m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  58m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  58m (x8 over 58m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    58m (x8 over 58m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   RegisteredNode           57m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeNotReady             45m (x2 over 56m)  kubelet          Node minikube status is now: NodeNotReady
  Warning  ContainerGCFailed        45m                kubelet          rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
  Normal   RegisteredNode           42m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.000000] Call Trace:
[  +0.000000]  fpu__init_system+0x11f/0x158
[  +0.000000]  ? cpu_set_bug_bits.constprop.0+0x1f9/0x210
[  +0.000000]  early_cpu_init+0x155/0x174
[  +0.000000]  setup_arch+0xcf/0xa2e
[  +0.000000]  ? lockdown_lsm_init+0x21/0x25
[  +0.000000]  start_kernel+0x68/0x56a
[  +0.000000]  ? copy_bootdata+0x1d/0x5d
[  +0.000000]  x86_64_start_reservations+0x24/0x26
[  +0.000000]  x86_64_start_kernel+0x75/0x79
[  +0.000000]  secondary_startup_64+0xa4/0xb0
[  +0.000000] ---[ end trace d530f4ccb39c6cdb ]---
[  +0.000000] CPUID[0d, 00]: eax=000000e7 ebx=00000a80 ecx=00000a80 edx=00000000
[  +0.000000] CPUID[0d, 01]: eax=0000000f ebx=00000a80 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 02]: eax=00000100 ebx=00000240 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 03]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 04]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 05]: eax=00000040 ebx=00000440 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 06]: eax=00000200 ebx=00000480 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 07]: eax=00000400 ebx=00000680 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 08]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 09]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 0a]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 0b]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 0c]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 0d]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 0e]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 0f]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 10]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 11]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 12]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] CPUID[0d, 13]: eax=00000000 ebx=00000000 ecx=00000000 edx=00000000
[  +0.000000] core: CPUID marked event: 'cpu cycles' unavailable
[  +0.000100] core: CPUID marked event: 'instructions' unavailable
[  +0.000103] core: CPUID marked event: 'bus cycles' unavailable
[  +0.000100] core: CPUID marked event: 'cache references' unavailable
[  +0.000108] core: CPUID marked event: 'cache misses' unavailable
[  +0.000103] core: CPUID marked event: 'branch instructions' unavailable
[  +0.000113] core: CPUID marked event: 'branch misses' unavailable
[  +0.559754] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000038] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000063] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000035] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000037] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000060] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000036] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000036] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000045] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.311542] piix4_smbus 0000:00:07.3: SMBus Host Controller not enabled!
[  +0.604782] sd 32:0:0:0: [sda] Assuming drive cache: write through
[Jul20 19:18] kauditd_printk_skb: 18 callbacks suppressed
[Jul20 19:19] kauditd_printk_skb: 20 callbacks suppressed
[Jul23 00:09] [drm:drm_atomic_helper_wait_for_dependencies [drm_kms_helper]] *ERROR* [CRTC:38:crtc-0] flip_done timed out
[ +10.240005] [drm:drm_atomic_helper_wait_for_dependencies [drm_kms_helper]] *ERROR* [PLANE:34:plane-0] flip_done timed out
[Jul23 09:21] [drm:drm_atomic_helper_wait_for_dependencies [drm_kms_helper]] *ERROR* [CRTC:38:crtc-0] flip_done timed out
[ +10.239931] [drm:drm_atomic_helper_wait_for_dependencies [drm_kms_helper]] *ERROR* [PLANE:34:plane-0] flip_done timed out
[Jul23 22:45] [drm:drm_atomic_helper_wait_for_dependencies [drm_kms_helper]] *ERROR* [CRTC:38:crtc-0] flip_done timed out
[ +10.239884] [drm:drm_atomic_helper_wait_for_dependencies [drm_kms_helper]] *ERROR* [PLANE:34:plane-0] flip_done timed out
[Jul24 16:38] kauditd_printk_skb: 5 callbacks suppressed
[  +1.266160] Started bpfilter

* 
* ==> etcd [601aa25927b8] <==
* {"level":"info","ts":"2023-07-25T10:01:23.692Z","caller":"traceutil/trace.go:171","msg":"trace[51145123] transaction","detail":"{read_only:false; response_revision:51344; number_of_response:1; }","duration":"121.497172ms","start":"2023-07-25T10:01:23.571Z","end":"2023-07-25T10:01:23.692Z","steps":["trace[51145123] 'process raft request'  (duration: 106.23753ms)","trace[51145123] 'compare'  (duration: 14.869292ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:23.966Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"132.273342ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:340"}
{"level":"info","ts":"2023-07-25T10:01:23.966Z","caller":"traceutil/trace.go:171","msg":"trace[794695931] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:51344; }","duration":"132.344253ms","start":"2023-07-25T10:01:23.834Z","end":"2023-07-25T10:01:23.966Z","steps":["trace[794695931] 'range keys from in-memory index tree'  (duration: 132.193796ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.140Z","caller":"traceutil/trace.go:171","msg":"trace[1220789398] transaction","detail":"{read_only:false; response_revision:51346; number_of_response:1; }","duration":"163.735861ms","start":"2023-07-25T10:01:23.976Z","end":"2023-07-25T10:01:24.140Z","steps":["trace[1220789398] 'process raft request'  (duration: 111.039749ms)","trace[1220789398] 'compare'  (duration: 52.537364ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:01:24.288Z","caller":"traceutil/trace.go:171","msg":"trace[70788002] linearizableReadLoop","detail":"{readStateIndex:64174; appliedIndex:64173; }","duration":"139.185402ms","start":"2023-07-25T10:01:24.149Z","end":"2023-07-25T10:01:24.288Z","steps":["trace[70788002] 'read index received'  (duration: 98.098673ms)","trace[70788002] 'applied index is now lower than readState.Index'  (duration: 41.086266ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:01:24.288Z","caller":"traceutil/trace.go:171","msg":"trace[1055597171] transaction","detail":"{read_only:false; response_revision:51349; number_of_response:1; }","duration":"140.873802ms","start":"2023-07-25T10:01:24.148Z","end":"2023-07-25T10:01:24.288Z","steps":["trace[1055597171] 'process raft request'  (duration: 99.59466ms)","trace[1055597171] 'compare'  (duration: 40.842577ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:24.289Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"139.598164ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:480"}
{"level":"warn","ts":"2023-07-25T10:01:24.289Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"139.334632ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/minikube.17751358b4ddeeac\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-07-25T10:01:24.289Z","caller":"traceutil/trace.go:171","msg":"trace[1145448917] range","detail":"{range_begin:/registry/events/default/minikube.17751358b4ddeeac; range_end:; response_count:0; response_revision:51349; }","duration":"139.429021ms","start":"2023-07-25T10:01:24.150Z","end":"2023-07-25T10:01:24.289Z","steps":["trace[1145448917] 'agreement among raft nodes before linearized reading'  (duration: 139.306052ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.289Z","caller":"traceutil/trace.go:171","msg":"trace[1172842055] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:51349; }","duration":"139.644321ms","start":"2023-07-25T10:01:24.149Z","end":"2023-07-25T10:01:24.289Z","steps":["trace[1172842055] 'agreement among raft nodes before linearized reading'  (duration: 139.546604ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.458Z","caller":"traceutil/trace.go:171","msg":"trace[1418843057] transaction","detail":"{read_only:false; response_revision:51351; number_of_response:1; }","duration":"161.162845ms","start":"2023-07-25T10:01:24.297Z","end":"2023-07-25T10:01:24.458Z","steps":["trace[1418843057] 'process raft request'  (duration: 160.956131ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.467Z","caller":"traceutil/trace.go:171","msg":"trace[1608414571] transaction","detail":"{read_only:false; response_revision:51352; number_of_response:1; }","duration":"160.506636ms","start":"2023-07-25T10:01:24.307Z","end":"2023-07-25T10:01:24.467Z","steps":["trace[1608414571] 'process raft request'  (duration: 160.247213ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.738Z","caller":"traceutil/trace.go:171","msg":"trace[695747885] transaction","detail":"{read_only:false; response_revision:51355; number_of_response:1; }","duration":"188.972449ms","start":"2023-07-25T10:01:24.549Z","end":"2023-07-25T10:01:24.738Z","steps":["trace[695747885] 'process raft request'  (duration: 188.840874ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.762Z","caller":"traceutil/trace.go:171","msg":"trace[666360082] linearizableReadLoop","detail":"{readStateIndex:64181; appliedIndex:64180; }","duration":"188.292944ms","start":"2023-07-25T10:01:24.573Z","end":"2023-07-25T10:01:24.762Z","steps":["trace[666360082] 'read index received'  (duration: 164.7883ms)","trace[666360082] 'applied index is now lower than readState.Index'  (duration: 23.504319ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:01:24.762Z","caller":"traceutil/trace.go:171","msg":"trace[433750104] transaction","detail":"{read_only:false; response_revision:51356; number_of_response:1; }","duration":"212.596963ms","start":"2023-07-25T10:01:24.549Z","end":"2023-07-25T10:01:24.762Z","steps":["trace[433750104] 'process raft request'  (duration: 212.526237ms)"],"step_count":1}
{"level":"warn","ts":"2023-07-25T10:01:24.762Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.505446ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/wp-pv-claim\" ","response":"range_response_count:1 size:1686"}
{"level":"info","ts":"2023-07-25T10:01:24.762Z","caller":"traceutil/trace.go:171","msg":"trace[735691806] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/wp-pv-claim; range_end:; response_count:1; response_revision:51356; }","duration":"188.859244ms","start":"2023-07-25T10:01:24.573Z","end":"2023-07-25T10:01:24.762Z","steps":["trace[735691806] 'agreement among raft nodes before linearized reading'  (duration: 188.355083ms)"],"step_count":1}
{"level":"warn","ts":"2023-07-25T10:01:24.965Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.40028ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-scheduler-minikube.1775133f9674463c\" ","response":"range_response_count:1 size:764"}
{"level":"warn","ts":"2023-07-25T10:01:24.965Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.737451ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-763c93b8-8450-4723-9dbd-ac8374c92ed6\" ","response":"range_response_count:1 size:1204"}
{"level":"info","ts":"2023-07-25T10:01:24.965Z","caller":"traceutil/trace.go:171","msg":"trace[2068653610] range","detail":"{range_begin:/registry/persistentvolumes/pvc-763c93b8-8450-4723-9dbd-ac8374c92ed6; range_end:; response_count:1; response_revision:51356; }","duration":"113.778855ms","start":"2023-07-25T10:01:24.851Z","end":"2023-07-25T10:01:24.965Z","steps":["trace[2068653610] 'range keys from in-memory index tree'  (duration: 113.601103ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.966Z","caller":"traceutil/trace.go:171","msg":"trace[1202047059] range","detail":"{range_begin:/registry/events/kube-system/kube-scheduler-minikube.1775133f9674463c; range_end:; response_count:1; response_revision:51356; }","duration":"113.463099ms","start":"2023-07-25T10:01:24.852Z","end":"2023-07-25T10:01:24.965Z","steps":["trace[1202047059] 'range keys from in-memory index tree'  (duration: 113.330546ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:24.997Z","caller":"traceutil/trace.go:171","msg":"trace[32761550] transaction","detail":"{read_only:false; response_revision:51357; number_of_response:1; }","duration":"144.035917ms","start":"2023-07-25T10:01:24.853Z","end":"2023-07-25T10:01:24.997Z","steps":["trace[32761550] 'process raft request'  (duration: 143.90963ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:25.876Z","caller":"traceutil/trace.go:171","msg":"trace[2018378833] transaction","detail":"{read_only:false; response_revision:51373; number_of_response:1; }","duration":"125.841693ms","start":"2023-07-25T10:01:25.750Z","end":"2023-07-25T10:01:25.876Z","steps":["trace[2018378833] 'process raft request'  (duration: 125.370309ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:26.778Z","caller":"traceutil/trace.go:171","msg":"trace[183884641] transaction","detail":"{read_only:false; response_revision:51394; number_of_response:1; }","duration":"631.281861ms","start":"2023-07-25T10:01:26.147Z","end":"2023-07-25T10:01:26.778Z","steps":["trace[183884641] 'process raft request'  (duration: 631.157706ms)"],"step_count":1}
{"level":"warn","ts":"2023-07-25T10:01:26.987Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-07-25T10:01:26.147Z","time spent":"631.518787ms","remote":"127.0.0.1:38790","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":803,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/etcd-minikube.177513fa21269d15\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/etcd-minikube.177513fa21269d15\" value_size:726 lease:8128022664011855117 >> failure:<>"}
{"level":"warn","ts":"2023-07-25T10:01:27.234Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.794441ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022664011855221 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/etcd-minikube.177513399eaf0407\" mod_revision:51389 > success:<request_put:<key:\"/registry/events/kube-system/etcd-minikube.177513399eaf0407\" value_size:551 lease:8128022664011855117 >> failure:<request_range:<key:\"/registry/events/kube-system/etcd-minikube.177513399eaf0407\" > >>","response":"size:18"}
{"level":"info","ts":"2023-07-25T10:01:27.234Z","caller":"traceutil/trace.go:171","msg":"trace[541578626] transaction","detail":"{read_only:false; response_revision:51402; number_of_response:1; }","duration":"186.380903ms","start":"2023-07-25T10:01:27.048Z","end":"2023-07-25T10:01:27.234Z","steps":["trace[541578626] 'process raft request'  (duration: 81.181451ms)","trace[541578626] 'compare'  (duration: 104.69379ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:27.618Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"282.137977ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022664011855225 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/coredns-5d78c9869d-2sjp8.177513fb9a433595\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/coredns-5d78c9869d-2sjp8.177513fb9a433595\" value_size:632 lease:8128022664011855117 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2023-07-25T10:01:27.619Z","caller":"traceutil/trace.go:171","msg":"trace[1778666675] transaction","detail":"{read_only:false; response_revision:51405; number_of_response:1; }","duration":"344.208709ms","start":"2023-07-25T10:01:27.274Z","end":"2023-07-25T10:01:27.619Z","steps":["trace[1778666675] 'process raft request'  (duration: 61.88993ms)","trace[1778666675] 'compare'  (duration: 282.007956ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:27.619Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-07-25T10:01:27.274Z","time spent":"344.282804ms","remote":"127.0.0.1:38790","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":720,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/coredns-5d78c9869d-2sjp8.177513fb9a433595\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/coredns-5d78c9869d-2sjp8.177513fb9a433595\" value_size:632 lease:8128022664011855117 >> failure:<>"}
{"level":"info","ts":"2023-07-25T10:01:27.923Z","caller":"traceutil/trace.go:171","msg":"trace[1693066723] transaction","detail":"{read_only:false; response_revision:51407; number_of_response:1; }","duration":"287.70005ms","start":"2023-07-25T10:01:27.635Z","end":"2023-07-25T10:01:27.923Z","steps":["trace[1693066723] 'process raft request'  (duration: 261.348657ms)","trace[1693066723] 'compare'  (duration: 26.250292ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:27.923Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"244.629596ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/logging/kibana-79c4f7d4c8-k7xpv\" ","response":"range_response_count:1 size:3062"}
{"level":"info","ts":"2023-07-25T10:01:27.923Z","caller":"traceutil/trace.go:171","msg":"trace[1131647716] range","detail":"{range_begin:/registry/pods/logging/kibana-79c4f7d4c8-k7xpv; range_end:; response_count:1; response_revision:51407; }","duration":"244.666655ms","start":"2023-07-25T10:01:27.679Z","end":"2023-07-25T10:01:27.923Z","steps":["trace[1131647716] 'agreement among raft nodes before linearized reading'  (duration: 244.550076ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:27.923Z","caller":"traceutil/trace.go:171","msg":"trace[1934862615] linearizableReadLoop","detail":"{readStateIndex:64232; appliedIndex:64231; }","duration":"244.495287ms","start":"2023-07-25T10:01:27.679Z","end":"2023-07-25T10:01:27.923Z","steps":["trace[1934862615] 'read index received'  (duration: 218.10924ms)","trace[1934862615] 'applied index is now lower than readState.Index'  (duration: 26.385683ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:01:28.109Z","caller":"traceutil/trace.go:171","msg":"trace[895484340] transaction","detail":"{read_only:false; response_revision:51415; number_of_response:1; }","duration":"113.341424ms","start":"2023-07-25T10:01:27.996Z","end":"2023-07-25T10:01:28.109Z","steps":["trace[895484340] 'process raft request'  (duration: 41.010338ms)","trace[895484340] 'compare'  (duration: 72.257292ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:01:28.963Z","caller":"traceutil/trace.go:171","msg":"trace[218390190] transaction","detail":"{read_only:false; response_revision:51454; number_of_response:1; }","duration":"177.051118ms","start":"2023-07-25T10:01:28.786Z","end":"2023-07-25T10:01:28.963Z","steps":["trace[218390190] 'process raft request'  (duration: 176.952825ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:29.275Z","caller":"traceutil/trace.go:171","msg":"trace[1865675070] transaction","detail":"{read_only:false; response_revision:51462; number_of_response:1; }","duration":"109.026939ms","start":"2023-07-25T10:01:29.166Z","end":"2023-07-25T10:01:29.275Z","steps":["trace[1865675070] 'process raft request'  (duration: 79.820719ms)","trace[1865675070] 'compare'  (duration: 29.051809ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:29.762Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.649405ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022664011855350 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" value_size:717 lease:8128022664011855117 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2023-07-25T10:01:29.763Z","caller":"traceutil/trace.go:171","msg":"trace[458760134] transaction","detail":"{read_only:false; response_revision:51465; number_of_response:1; }","duration":"443.166954ms","start":"2023-07-25T10:01:29.319Z","end":"2023-07-25T10:01:29.763Z","steps":["trace[458760134] 'process raft request'  (duration: 249.349869ms)","trace[458760134] 'compare'  (duration: 193.538984ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:29.763Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-07-25T10:01:29.319Z","time spent":"443.253145ms","remote":"127.0.0.1:38790","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":813,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" value_size:717 lease:8128022664011855117 >> failure:<>"}
{"level":"warn","ts":"2023-07-25T10:01:30.188Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.267568ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022664011855352 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" mod_revision:51465 > success:<request_put:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" value_size:717 lease:8128022664011855117 >> failure:<request_range:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" > >>","response":"size:18"}
{"level":"info","ts":"2023-07-25T10:01:30.188Z","caller":"traceutil/trace.go:171","msg":"trace[1570615208] transaction","detail":"{read_only:false; response_revision:51466; number_of_response:1; }","duration":"421.627506ms","start":"2023-07-25T10:01:29.766Z","end":"2023-07-25T10:01:30.188Z","steps":["trace[1570615208] 'process raft request'  (duration: 302.233703ms)","trace[1570615208] 'compare'  (duration: 119.181084ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:30.188Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-07-25T10:01:29.766Z","time spent":"421.681157ms","remote":"127.0.0.1:38790","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":813,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" mod_revision:51465 > success:<request_put:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" value_size:717 lease:8128022664011855117 >> failure:<request_range:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.1775140c94609942\" > >"}
{"level":"warn","ts":"2023-07-25T10:01:30.751Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"436.748961ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022664011855356 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/storage-provisioner.1775134efec7cbe0\" mod_revision:51463 > success:<request_put:<key:\"/registry/events/kube-system/storage-provisioner.1775134efec7cbe0\" value_size:680 lease:8128022664011855117 >> failure:<request_range:<key:\"/registry/events/kube-system/storage-provisioner.1775134efec7cbe0\" > >>","response":"size:18"}
{"level":"info","ts":"2023-07-25T10:01:30.751Z","caller":"traceutil/trace.go:171","msg":"trace[1239934229] transaction","detail":"{read_only:false; response_revision:51468; number_of_response:1; }","duration":"514.223543ms","start":"2023-07-25T10:01:30.237Z","end":"2023-07-25T10:01:30.751Z","steps":["trace[1239934229] 'process raft request'  (duration: 77.369681ms)","trace[1239934229] 'compare'  (duration: 436.609426ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:30.751Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-07-25T10:01:30.237Z","time spent":"514.257693ms","remote":"127.0.0.1:38790","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":763,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/storage-provisioner.1775134efec7cbe0\" mod_revision:51463 > success:<request_put:<key:\"/registry/events/kube-system/storage-provisioner.1775134efec7cbe0\" value_size:680 lease:8128022664011855117 >> failure:<request_range:<key:\"/registry/events/kube-system/storage-provisioner.1775134efec7cbe0\" > >"}
{"level":"info","ts":"2023-07-25T10:01:33.359Z","caller":"traceutil/trace.go:171","msg":"trace[1103756040] transaction","detail":"{read_only:false; response_revision:51469; number_of_response:1; }","duration":"204.668258ms","start":"2023-07-25T10:01:33.155Z","end":"2023-07-25T10:01:33.359Z","steps":["trace[1103756040] 'process raft request'  (duration: 204.559716ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:33.534Z","caller":"traceutil/trace.go:171","msg":"trace[1865433631] transaction","detail":"{read_only:false; response_revision:51470; number_of_response:1; }","duration":"156.929556ms","start":"2023-07-25T10:01:33.377Z","end":"2023-07-25T10:01:33.534Z","steps":["trace[1865433631] 'process raft request'  (duration: 151.766566ms)"],"step_count":1}
{"level":"warn","ts":"2023-07-25T10:01:35.356Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"243.54365ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128022664011855418 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.177514103330d906\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/minikube.177514103330d906\" value_size:446 lease:8128022664011855415 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2023-07-25T10:01:35.356Z","caller":"traceutil/trace.go:171","msg":"trace[1624320659] transaction","detail":"{read_only:false; response_revision:51475; number_of_response:1; }","duration":"262.414417ms","start":"2023-07-25T10:01:35.093Z","end":"2023-07-25T10:01:35.356Z","steps":["trace[1624320659] 'process raft request'  (duration: 18.714547ms)","trace[1624320659] 'compare'  (duration: 243.407836ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:01:35.356Z","caller":"traceutil/trace.go:171","msg":"trace[31584480] linearizableReadLoop","detail":"{readStateIndex:64302; appliedIndex:64301; }","duration":"260.813574ms","start":"2023-07-25T10:01:35.095Z","end":"2023-07-25T10:01:35.356Z","steps":["trace[31584480] 'read index received'  (duration: 17.044992ms)","trace[31584480] 'applied index is now lower than readState.Index'  (duration: 243.638254ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:01:35.356Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"261.214964ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpointslicemirroring-controller\" ","response":"range_response_count:1 size:233"}
{"level":"info","ts":"2023-07-25T10:01:35.356Z","caller":"traceutil/trace.go:171","msg":"trace[1657624554] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpointslicemirroring-controller; range_end:; response_count:1; response_revision:51475; }","duration":"261.244654ms","start":"2023-07-25T10:01:35.095Z","end":"2023-07-25T10:01:35.356Z","steps":["trace[1657624554] 'agreement among raft nodes before linearized reading'  (duration: 260.930949ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:01:37.938Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-07-25T10:01:37.939Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2023/07/25 10:01:37 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"info","ts":"2023-07-25T10:01:37.950Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-07-25T10:01:38.090Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-07-25T10:01:38.093Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-07-25T10:01:38.093Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [af629843e3ef] <==
* {"level":"warn","ts":"2023-07-25T10:12:59.864Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"209.86959ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/logging/fluentd-kwbpj\" ","response":"range_response_count:1 size:4729"}
{"level":"info","ts":"2023-07-25T10:12:59.864Z","caller":"traceutil/trace.go:171","msg":"trace[171812778] range","detail":"{range_begin:/registry/pods/logging/fluentd-kwbpj; range_end:; response_count:1; response_revision:52084; }","duration":"210.010152ms","start":"2023-07-25T10:12:59.654Z","end":"2023-07-25T10:12:59.864Z","steps":["trace[171812778] 'agreement among raft nodes before linearized reading'  (duration: 209.795278ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:12:59.990Z","caller":"traceutil/trace.go:171","msg":"trace[188881086] transaction","detail":"{read_only:false; response_revision:52085; number_of_response:1; }","duration":"121.205225ms","start":"2023-07-25T10:12:59.869Z","end":"2023-07-25T10:12:59.990Z","steps":["trace[188881086] 'process raft request'  (duration: 40.614904ms)","trace[188881086] 'compare'  (duration: 80.291265ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:13:01.917Z","caller":"traceutil/trace.go:171","msg":"trace[1622225107] transaction","detail":"{read_only:false; response_revision:52091; number_of_response:1; }","duration":"119.008522ms","start":"2023-07-25T10:13:01.798Z","end":"2023-07-25T10:13:01.917Z","steps":["trace[1622225107] 'process raft request'  (duration: 118.906893ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:14:36.756Z","caller":"traceutil/trace.go:171","msg":"trace[1036540543] transaction","detail":"{read_only:false; response_revision:52194; number_of_response:1; }","duration":"139.360008ms","start":"2023-07-25T10:14:36.616Z","end":"2023-07-25T10:14:36.756Z","steps":["trace[1036540543] 'process raft request'  (duration: 138.934524ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:17:03.824Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52008}
{"level":"info","ts":"2023-07-25T10:17:03.863Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":52008,"took":"39.213381ms","hash":3176935876}
{"level":"info","ts":"2023-07-25T10:17:03.864Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3176935876,"revision":52008,"compact-revision":51753}
{"level":"info","ts":"2023-07-25T10:22:03.827Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52328}
{"level":"info","ts":"2023-07-25T10:22:03.828Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":52328,"took":"709.819µs","hash":1324439347}
{"level":"info","ts":"2023-07-25T10:22:03.828Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1324439347,"revision":52328,"compact-revision":52008}
{"level":"info","ts":"2023-07-25T10:23:48.877Z","caller":"traceutil/trace.go:171","msg":"trace[2023053613] transaction","detail":"{read_only:false; response_revision:52707; number_of_response:1; }","duration":"134.147588ms","start":"2023-07-25T10:23:48.743Z","end":"2023-07-25T10:23:48.877Z","steps":["trace[2023053613] 'process raft request'  (duration: 79.372699ms)","trace[2023053613] 'compare'  (duration: 54.695496ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:24:10.860Z","caller":"traceutil/trace.go:171","msg":"trace[1733679870] transaction","detail":"{read_only:false; response_revision:52746; number_of_response:1; }","duration":"304.485642ms","start":"2023-07-25T10:24:10.556Z","end":"2023-07-25T10:24:10.860Z","steps":["trace[1733679870] 'process raft request'  (duration: 304.387634ms)"],"step_count":1}
{"level":"warn","ts":"2023-07-25T10:24:10.860Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-07-25T10:24:10.556Z","time spent":"304.569411ms","remote":"127.0.0.1:39948","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:52739 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-07-25T10:24:28.817Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.250714ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/wp-pv-claim\" ","response":"range_response_count:1 size:1686"}
{"level":"info","ts":"2023-07-25T10:24:28.818Z","caller":"traceutil/trace.go:171","msg":"trace[57742799] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/wp-pv-claim; range_end:; response_count:1; response_revision:52784; }","duration":"146.335651ms","start":"2023-07-25T10:24:28.671Z","end":"2023-07-25T10:24:28.818Z","steps":["trace[57742799] 'range keys from in-memory index tree'  (duration: 146.154559ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:24:53.190Z","caller":"traceutil/trace.go:171","msg":"trace[35649113] transaction","detail":"{read_only:false; response_revision:52813; number_of_response:1; }","duration":"138.557931ms","start":"2023-07-25T10:24:53.051Z","end":"2023-07-25T10:24:53.190Z","steps":["trace[35649113] 'process raft request'  (duration: 75.160937ms)","trace[35649113] 'compare'  (duration: 63.071613ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:25:46.310Z","caller":"traceutil/trace.go:171","msg":"trace[1725245943] transaction","detail":"{read_only:false; response_revision:52871; number_of_response:1; }","duration":"147.789727ms","start":"2023-07-25T10:25:46.162Z","end":"2023-07-25T10:25:46.310Z","steps":["trace[1725245943] 'process raft request'  (duration: 92.596048ms)","trace[1725245943] 'compare'  (duration: 54.893601ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:27:03.831Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52586}
{"level":"info","ts":"2023-07-25T10:27:03.832Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":52586,"took":"667.571µs","hash":2667356510}
{"level":"info","ts":"2023-07-25T10:27:03.832Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2667356510,"revision":52586,"compact-revision":52328}
{"level":"info","ts":"2023-07-25T10:30:12.589Z","caller":"traceutil/trace.go:171","msg":"trace[298908026] transaction","detail":"{read_only:false; response_revision:53119; number_of_response:1; }","duration":"145.219653ms","start":"2023-07-25T10:30:12.444Z","end":"2023-07-25T10:30:12.589Z","steps":["trace[298908026] 'process raft request'  (duration: 145.130065ms)"],"step_count":1}
{"level":"warn","ts":"2023-07-25T10:31:51.862Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.222125ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-07-25T10:31:52.005Z","caller":"traceutil/trace.go:171","msg":"trace[938257225] range","detail":"{range_begin:/registry/certificatesigningrequests/; range_end:/registry/certificatesigningrequests0; response_count:0; response_revision:53242; }","duration":"194.320436ms","start":"2023-07-25T10:31:51.668Z","end":"2023-07-25T10:31:51.862Z","steps":["trace[938257225] 'count revisions from in-memory index tree'  (duration: 194.155203ms)"],"step_count":1}
{"level":"warn","ts":"2023-07-25T10:31:52.005Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-07-25T10:31:51.668Z","time spent":"336.79656ms","remote":"127.0.0.1:39972","response type":"/etcdserverpb.KV/Range","request count":0,"request size":80,"response count":0,"response size":30,"request content":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true "}
{"level":"info","ts":"2023-07-25T10:32:03.836Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52946}
{"level":"info","ts":"2023-07-25T10:32:03.929Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":52946,"took":"846.7µs","hash":1851894601}
{"level":"info","ts":"2023-07-25T10:32:03.929Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1851894601,"revision":52946,"compact-revision":52586}
{"level":"info","ts":"2023-07-25T10:33:22.434Z","caller":"traceutil/trace.go:171","msg":"trace[1892563931] linearizableReadLoop","detail":"{readStateIndex:66566; appliedIndex:66565; }","duration":"131.368867ms","start":"2023-07-25T10:33:22.302Z","end":"2023-07-25T10:33:22.434Z","steps":["trace[1892563931] 'read index received'  (duration: 49.648644ms)","trace[1892563931] 'applied index is now lower than readState.Index'  (duration: 81.719749ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:33:22.434Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.500358ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-07-25T10:33:22.434Z","caller":"traceutil/trace.go:171","msg":"trace[134852872] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:53333; }","duration":"131.655517ms","start":"2023-07-25T10:33:22.302Z","end":"2023-07-25T10:33:22.434Z","steps":["trace[134852872] 'agreement among raft nodes before linearized reading'  (duration: 131.44336ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:33:22.434Z","caller":"traceutil/trace.go:171","msg":"trace[2028572] transaction","detail":"{read_only:false; response_revision:53333; number_of_response:1; }","duration":"150.000832ms","start":"2023-07-25T10:33:22.284Z","end":"2023-07-25T10:33:22.434Z","steps":["trace[2028572] 'process raft request'  (duration: 67.847218ms)","trace[2028572] 'compare'  (duration: 81.582039ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:36:59.652Z","caller":"traceutil/trace.go:171","msg":"trace[1076453815] linearizableReadLoop","detail":"{readStateIndex:66795; appliedIndex:66794; }","duration":"135.909052ms","start":"2023-07-25T10:36:59.516Z","end":"2023-07-25T10:36:59.652Z","steps":["trace[1076453815] 'read index received'  (duration: 135.797494ms)","trace[1076453815] 'applied index is now lower than readState.Index'  (duration: 111.266µs)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:36:59.652Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.194441ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/wp-pv-claim\" ","response":"range_response_count:1 size:1686"}
{"level":"info","ts":"2023-07-25T10:36:59.653Z","caller":"traceutil/trace.go:171","msg":"trace[1092783400] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/wp-pv-claim; range_end:; response_count:1; response_revision:53518; }","duration":"136.238137ms","start":"2023-07-25T10:36:59.516Z","end":"2023-07-25T10:36:59.653Z","steps":["trace[1092783400] 'agreement among raft nodes before linearized reading'  (duration: 136.002391ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:36:59.653Z","caller":"traceutil/trace.go:171","msg":"trace[1033950756] transaction","detail":"{read_only:false; response_revision:53518; number_of_response:1; }","duration":"146.711896ms","start":"2023-07-25T10:36:59.506Z","end":"2023-07-25T10:36:59.653Z","steps":["trace[1033950756] 'process raft request'  (duration: 146.09004ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:37:03.840Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53264}
{"level":"info","ts":"2023-07-25T10:37:03.841Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":53264,"took":"705.737µs","hash":2748220599}
{"level":"info","ts":"2023-07-25T10:37:03.841Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2748220599,"revision":53264,"compact-revision":52946}
{"level":"info","ts":"2023-07-25T10:38:26.994Z","caller":"traceutil/trace.go:171","msg":"trace[1529448975] transaction","detail":"{read_only:false; response_revision:53602; number_of_response:1; }","duration":"103.720509ms","start":"2023-07-25T10:38:26.890Z","end":"2023-07-25T10:38:26.994Z","steps":["trace[1529448975] 'process raft request'  (duration: 103.626033ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:41:06.577Z","caller":"traceutil/trace.go:171","msg":"trace[650665387] transaction","detail":"{read_only:false; response_revision:53736; number_of_response:1; }","duration":"206.980057ms","start":"2023-07-25T10:41:06.370Z","end":"2023-07-25T10:41:06.577Z","steps":["trace[650665387] 'process raft request'  (duration: 135.439411ms)","trace[650665387] 'compare'  (duration: 71.439716ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:42:03.844Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53527}
{"level":"info","ts":"2023-07-25T10:42:03.845Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":53527,"took":"914.223µs","hash":2570383309}
{"level":"info","ts":"2023-07-25T10:42:03.845Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2570383309,"revision":53527,"compact-revision":53264}
{"level":"info","ts":"2023-07-25T10:42:37.459Z","caller":"traceutil/trace.go:171","msg":"trace[1778227406] linearizableReadLoop","detail":"{readStateIndex:67167; appliedIndex:67166; }","duration":"156.825855ms","start":"2023-07-25T10:42:37.302Z","end":"2023-07-25T10:42:37.459Z","steps":["trace[1778227406] 'read index received'  (duration: 133.527416ms)","trace[1778227406] 'applied index is now lower than readState.Index'  (duration: 23.297648ms)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:42:37.459Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"156.989883ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/wp-pv-claim\" ","response":"range_response_count:1 size:1686"}
{"level":"info","ts":"2023-07-25T10:42:37.459Z","caller":"traceutil/trace.go:171","msg":"trace[2016424398] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/wp-pv-claim; range_end:; response_count:1; response_revision:53817; }","duration":"157.153801ms","start":"2023-07-25T10:42:37.302Z","end":"2023-07-25T10:42:37.459Z","steps":["trace[2016424398] 'agreement among raft nodes before linearized reading'  (duration: 156.913913ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:42:37.460Z","caller":"traceutil/trace.go:171","msg":"trace[707858898] transaction","detail":"{read_only:false; response_revision:53817; number_of_response:1; }","duration":"181.149589ms","start":"2023-07-25T10:42:37.278Z","end":"2023-07-25T10:42:37.460Z","steps":["trace[707858898] 'process raft request'  (duration: 157.387848ms)","trace[707858898] 'compare'  (duration: 23.186197ms)"],"step_count":2}
{"level":"info","ts":"2023-07-25T10:42:52.238Z","caller":"traceutil/trace.go:171","msg":"trace[2030748671] transaction","detail":"{read_only:false; response_revision:53829; number_of_response:1; }","duration":"114.714575ms","start":"2023-07-25T10:42:52.123Z","end":"2023-07-25T10:42:52.238Z","steps":["trace[2030748671] 'process raft request'  (duration: 114.488053ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:43:42.085Z","caller":"traceutil/trace.go:171","msg":"trace[754149672] transaction","detail":"{read_only:false; response_revision:53903; number_of_response:1; }","duration":"157.684314ms","start":"2023-07-25T10:43:41.928Z","end":"2023-07-25T10:43:42.085Z","steps":["trace[754149672] 'process raft request'  (duration: 157.525333ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:43:42.086Z","caller":"traceutil/trace.go:171","msg":"trace[1523272013] transaction","detail":"{read_only:false; response_revision:53904; number_of_response:1; }","duration":"155.280454ms","start":"2023-07-25T10:43:41.931Z","end":"2023-07-25T10:43:42.086Z","steps":["trace[1523272013] 'process raft request'  (duration: 154.642424ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:43:42.872Z","caller":"traceutil/trace.go:171","msg":"trace[1227509515] linearizableReadLoop","detail":"{readStateIndex:67279; appliedIndex:67278; }","duration":"114.644228ms","start":"2023-07-25T10:43:42.757Z","end":"2023-07-25T10:43:42.872Z","steps":["trace[1227509515] 'read index received'  (duration: 114.470711ms)","trace[1227509515] 'applied index is now lower than readState.Index'  (duration: 173.25µs)"],"step_count":2}
{"level":"warn","ts":"2023-07-25T10:43:42.872Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.891068ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/default/\" range_end:\"/registry/secrets/default0\" limit:500 ","response":"range_response_count:2 size:1319"}
{"level":"info","ts":"2023-07-25T10:43:42.872Z","caller":"traceutil/trace.go:171","msg":"trace[1346807815] range","detail":"{range_begin:/registry/secrets/default/; range_end:/registry/secrets/default0; response_count:2; response_revision:53915; }","duration":"115.035317ms","start":"2023-07-25T10:43:42.757Z","end":"2023-07-25T10:43:42.872Z","steps":["trace[1346807815] 'agreement among raft nodes before linearized reading'  (duration: 114.764818ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:43:42.872Z","caller":"traceutil/trace.go:171","msg":"trace[793513800] transaction","detail":"{read_only:false; response_revision:53915; number_of_response:1; }","duration":"234.111424ms","start":"2023-07-25T10:43:42.638Z","end":"2023-07-25T10:43:42.872Z","steps":["trace[793513800] 'process raft request'  (duration: 233.348948ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:43:43.632Z","caller":"traceutil/trace.go:171","msg":"trace[300011948] transaction","detail":"{read_only:false; response_revision:53917; number_of_response:1; }","duration":"103.786023ms","start":"2023-07-25T10:43:43.528Z","end":"2023-07-25T10:43:43.632Z","steps":["trace[300011948] 'process raft request'  (duration: 103.371767ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:43:46.797Z","caller":"traceutil/trace.go:171","msg":"trace[1095144717] transaction","detail":"{read_only:false; response_revision:53920; number_of_response:1; }","duration":"183.465591ms","start":"2023-07-25T10:43:46.613Z","end":"2023-07-25T10:43:46.797Z","steps":["trace[1095144717] 'process raft request'  (duration: 183.380043ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:43:48.909Z","caller":"traceutil/trace.go:171","msg":"trace[1101225034] transaction","detail":"{read_only:false; response_revision:53924; number_of_response:1; }","duration":"106.520705ms","start":"2023-07-25T10:43:48.803Z","end":"2023-07-25T10:43:48.909Z","steps":["trace[1101225034] 'process raft request'  (duration: 106.422719ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:44:05.177Z","caller":"traceutil/trace.go:171","msg":"trace[1906887791] transaction","detail":"{read_only:false; response_revision:53969; number_of_response:1; }","duration":"113.909153ms","start":"2023-07-25T10:44:05.063Z","end":"2023-07-25T10:44:05.177Z","steps":["trace[1906887791] 'process raft request'  (duration: 113.812486ms)"],"step_count":1}
{"level":"info","ts":"2023-07-25T10:44:53.583Z","caller":"traceutil/trace.go:171","msg":"trace[1457460654] transaction","detail":"{read_only:false; response_revision:54028; number_of_response:1; }","duration":"146.372378ms","start":"2023-07-25T10:44:53.437Z","end":"2023-07-25T10:44:53.583Z","steps":["trace[1457460654] 'process raft request'  (duration: 146.108574ms)"],"step_count":1}

* 
* ==> kernel <==
*  10:44:56 up 4 days, 19:03,  0 users,  load average: 0.42, 0.40, 0.74
Linux minikube 5.4.0-81-generic #91-Ubuntu SMP Thu Jul 15 19:09:17 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [4548e9df158f] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 10:01:47.736559       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 10:01:47.755054       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 10:01:47.760548       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 10:01:47.780196       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 10:01:47.782477       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 10:01:47.872704       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0725 10:01:47.877094       1 logging.go:59] [core] [Channel #38 SubChannel #39] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [f9b42d9d9f37] <==
* Trace[923486475]: ---"Conversion done" 897ms (10:03:53.415)
Trace[923486475]: [1.047990606s] [1.047990606s] END
I0725 10:04:15.021594       1 trace.go:219] Trace[100913672]: "Update" accept:application/json, */*,audit-id:b7cd80b8-6fed-4994-aae3-3f37ccb6f741,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (25-Jul-2023 10:04:14.464) (total time: 557ms):
Trace[100913672]: ["GuaranteedUpdate etcd3" audit-id:b7cd80b8-6fed-4994-aae3-3f37ccb6f741,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 556ms (10:04:14.464)
Trace[100913672]:  ---"Txn call completed" 556ms (10:04:15.021)]
Trace[100913672]: [557.298139ms] [557.298139ms] END
I0725 10:04:17.158021       1 trace.go:219] Trace[2137621685]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (25-Jul-2023 10:04:16.242) (total time: 915ms):
Trace[2137621685]: ---"initial value restored" 304ms (10:04:16.546)
Trace[2137621685]: ---"Txn call completed" 561ms (10:04:17.157)
Trace[2137621685]: [915.501702ms] [915.501702ms] END
I0725 10:04:36.818001       1 trace.go:219] Trace[1497031910]: "Update" accept:application/json, */*,audit-id:90142b6e-8525-4375-9f25-32b68d2df4ce,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (25-Jul-2023 10:04:36.192) (total time: 625ms):
Trace[1497031910]: ["GuaranteedUpdate etcd3" audit-id:90142b6e-8525-4375-9f25-32b68d2df4ce,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 625ms (10:04:36.192)
Trace[1497031910]:  ---"Txn call completed" 624ms (10:04:36.817)]
Trace[1497031910]: [625.519617ms] [625.519617ms] END
I0725 10:04:41.300025       1 trace.go:219] Trace[1588239072]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6d10e8b2-6429-47b5-b9f1-6859ae93f47f,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (25-Jul-2023 10:04:40.727) (total time: 529ms):
Trace[1588239072]: [529.29838ms] [529.29838ms] END
I0725 10:04:55.167937       1 trace.go:219] Trace[688493448]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a5aee700-6800-4b31-9972-a7912603ffbf,client:192.168.49.2,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:resource,url:/api/v1/namespaces/default/persistentvolumeclaims/mysql-pv-claim,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (25-Jul-2023 10:04:54.454) (total time: 712ms):
Trace[688493448]: ---"About to write a response" 712ms (10:04:55.167)
Trace[688493448]: [712.917687ms] [712.917687ms] END
I0725 10:05:08.722419       1 trace.go:219] Trace[1172309320]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:af2d9ff4-b608-4c7c-b87a-963505388af3,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:LIST (25-Jul-2023 10:05:07.473) (total time: 1249ms):
Trace[1172309320]: ["List(recursive=true) etcd3" audit-id:af2d9ff4-b608-4c7c-b87a-963505388af3,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 1118ms (10:05:07.603)]
Trace[1172309320]: ---"Writing http response done" count:6 255ms (10:05:08.722)
Trace[1172309320]: [1.249165993s] [1.249165993s] END
I0725 10:05:08.722667       1 trace.go:219] Trace[1930305724]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3a636494-6d37-46e8-b7a0-753cd83d44f3,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:LIST (25-Jul-2023 10:05:07.473) (total time: 1249ms):
Trace[1930305724]: ["List(recursive=true) etcd3" audit-id:3a636494-6d37-46e8-b7a0-753cd83d44f3,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 1118ms (10:05:07.604)]
Trace[1930305724]: ---"Writing http response done" count:6 258ms (10:05:08.722)
Trace[1930305724]: [1.249548134s] [1.249548134s] END
I0725 10:05:11.490649       1 trace.go:219] Trace[464007083]: "Update" accept:application/json, */*,audit-id:6a7d1a24-9033-4c97-8ef4-6d4460755888,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (25-Jul-2023 10:05:10.894) (total time: 596ms):
Trace[464007083]: ["GuaranteedUpdate etcd3" audit-id:6a7d1a24-9033-4c97-8ef4-6d4460755888,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 596ms (10:05:10.894)
Trace[464007083]:  ---"Txn call completed" 595ms (10:05:11.490)]
Trace[464007083]: [596.455582ms] [596.455582ms] END
I0725 10:05:17.757224       1 trace.go:219] Trace[959534783]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (25-Jul-2023 10:05:16.247) (total time: 1510ms):
Trace[959534783]: ---"Transaction prepared" 969ms (10:05:17.217)
Trace[959534783]: ---"Txn call completed" 539ms (10:05:17.757)
Trace[959534783]: [1.510017057s] [1.510017057s] END
I0725 10:05:18.909514       1 trace.go:219] Trace[1783241528]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ea4ab2dd-dc78-4561-a48e-072b2d3dfa55,client:192.168.49.2,protocol:HTTP/2.0,resource:persistentvolumeclaims,scope:resource,url:/api/v1/namespaces/default/persistentvolumeclaims/wp-pv-claim,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (25-Jul-2023 10:05:18.205) (total time: 704ms):
Trace[1783241528]: ---"About to write a response" 704ms (10:05:18.909)
Trace[1783241528]: [704.175504ms] [704.175504ms] END
I0725 10:06:48.643837       1 trace.go:219] Trace[964053545]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5c5b2a3f-5e11-414a-9c15-6ce572d6f832,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/fluentd-jxmvh,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (25-Jul-2023 10:06:48.094) (total time: 548ms):
Trace[964053545]: ---"About to write a response" 470ms (10:06:48.565)
Trace[964053545]: [548.90667ms] [548.90667ms] END
I0725 10:06:52.220865       1 trace.go:219] Trace[740905929]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:8bc49fab-6a77-4cd4-b488-0dbaf7a437c2,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/daemon-set-controller/token,user-agent:kube-controller-manager/v1.27.3 (linux/amd64) kubernetes/25b4e43/kube-controller-manager,verb:POST (25-Jul-2023 10:06:51.700) (total time: 520ms):
Trace[740905929]: ---"About to store object in database" 142ms (10:06:51.900)
Trace[740905929]: ---"Write to database call succeeded" len:81 319ms (10:06:52.220)
Trace[740905929]: [520.754945ms] [520.754945ms] END
I0725 10:06:52.941874       1 trace.go:219] Trace[878019973]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d5170f85-6862-465b-80c1-7695616eeb25,client:192.168.49.2,protocol:HTTP/2.0,resource:daemonsets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/daemonsets/fluentd/status,user-agent:kube-controller-manager/v1.27.3 (linux/amd64) kubernetes/25b4e43/system:serviceaccount:kube-system:daemon-set-controller,verb:PUT (25-Jul-2023 10:06:52.247) (total time: 693ms):
Trace[878019973]: ---"Conversion done" 255ms (10:06:52.503)
Trace[878019973]: [693.879813ms] [693.879813ms] END
I0725 10:10:57.075504       1 trace.go:219] Trace[698799948]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (25-Jul-2023 10:10:56.410) (total time: 530ms):
Trace[698799948]: ---"Transaction prepared" 84ms (10:10:56.494)
Trace[698799948]: ---"Txn call completed" 445ms (10:10:56.940)
Trace[698799948]: [530.081656ms] [530.081656ms] END
I0725 10:12:23.074217       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0725 10:12:23.127279       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0725 10:18:56.044752       1 trace.go:219] Trace[1376616301]: "Get" accept:application/json, */*,audit-id:bf8ce60b-04f0-4742-9290-0789bdbb832c,client:192.168.49.1,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/fluentd-jxmvh/log,user-agent:kubectl/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:CONNECT (25-Jul-2023 10:18:54.786) (total time: 1257ms):
Trace[1376616301]: ---"About to write a response" 157ms (10:18:54.944)
Trace[1376616301]: ---"Writing http response done" 1100ms (10:18:56.044)
Trace[1376616301]: [1.257732526s] [1.257732526s] END
I0725 10:43:39.052543       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0725 10:43:39.125347       1 controller.go:624] quota admission added evaluator for: replicasets.apps

* 
* ==> kube-controller-manager [2632a1396495] <==
* I0725 10:02:19.917547       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0725 10:02:19.917552       1 shared_informer.go:318] Caches are synced for cidrallocator
I0725 10:02:19.917636       1 shared_informer.go:318] Caches are synced for TTL
I0725 10:02:19.956046       1 shared_informer.go:318] Caches are synced for namespace
I0725 10:02:19.956650       1 shared_informer.go:318] Caches are synced for cronjob
I0725 10:02:19.979238       1 shared_informer.go:318] Caches are synced for service account
I0725 10:02:19.979602       1 shared_informer.go:318] Caches are synced for disruption
I0725 10:02:19.983126       1 shared_informer.go:318] Caches are synced for ReplicationController
I0725 10:02:19.983400       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0725 10:02:20.002033       1 shared_informer.go:318] Caches are synced for taint
I0725 10:02:20.159837       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0725 10:02:20.160724       1 shared_informer.go:318] Caches are synced for persistent volume
I0725 10:02:20.160899       1 shared_informer.go:318] Caches are synced for HPA
I0725 10:02:20.161012       1 shared_informer.go:318] Caches are synced for GC
I0725 10:02:20.161098       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0725 10:02:20.161197       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0725 10:02:20.161289       1 shared_informer.go:318] Caches are synced for PVC protection
I0725 10:02:20.161358       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0725 10:02:20.161420       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0725 10:02:20.161498       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0725 10:02:20.161564       1 taint_manager.go:211] "Sending events to api server"
I0725 10:02:20.161862       1 shared_informer.go:318] Caches are synced for attach detach
I0725 10:02:20.178594       1 shared_informer.go:318] Caches are synced for ephemeral
I0725 10:02:20.178742       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0725 10:02:20.188426       1 shared_informer.go:318] Caches are synced for deployment
I0725 10:02:20.188851       1 shared_informer.go:318] Caches are synced for crt configmap
I0725 10:02:20.189044       1 shared_informer.go:318] Caches are synced for daemon sets
I0725 10:02:20.189070       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0725 10:02:20.189131       1 shared_informer.go:318] Caches are synced for PV protection
I0725 10:02:20.189148       1 shared_informer.go:318] Caches are synced for expand
I0725 10:02:20.192578       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0725 10:02:20.192858       1 shared_informer.go:318] Caches are synced for job
I0725 10:02:20.192941       1 shared_informer.go:318] Caches are synced for stateful set
I0725 10:02:20.205563       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0725 10:02:20.206064       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0725 10:02:20.206141       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0725 10:02:20.224019       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0725 10:02:20.243759       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0725 10:02:20.276585       1 shared_informer.go:318] Caches are synced for resource quota
I0725 10:02:20.276731       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0725 10:02:20.305594       1 shared_informer.go:318] Caches are synced for resource quota
I0725 10:02:20.345989       1 shared_informer.go:318] Caches are synced for endpoint
I0725 10:02:20.375147       1 shared_informer.go:318] Caches are synced for garbage collector
I0725 10:02:20.420411       1 shared_informer.go:318] Caches are synced for garbage collector
I0725 10:02:20.420577       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0725 10:12:23.714270       1 event.go:307] "Event occurred" object="logging/fluentd" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fluentd-kwbpj"
I0725 10:23:38.945184       1 event.go:307] "Event occurred" object="kube-system/fluentd" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: fluentd-jxmvh"
I0725 10:23:41.255525       1 event.go:307] "Event occurred" object="kube-system/fluentd" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fluentd-2kdbl"
I0725 10:24:10.359839       1 event.go:307] "Event occurred" object="kube-system/fluentd" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: fluentd-2kdbl"
I0725 10:24:11.998073       1 event.go:307] "Event occurred" object="kube-system/fluentd" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fluentd-vtfqg"
I0725 10:30:11.986041       1 event.go:307] "Event occurred" object="kube-system/fluentd" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: fluentd-vtfqg"
I0725 10:30:13.385743       1 event.go:307] "Event occurred" object="kube-system/fluentd" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fluentd-rhj8j"
I0725 10:43:39.166653       1 event.go:307] "Event occurred" object="default/wordpress-mysql" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set wordpress-mysql-7cc886fc7c to 0 from 1"
I0725 10:43:39.166807       1 event.go:307] "Event occurred" object="default/wordpress" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set wordpress-fcfd6d499 to 0 from 1"
I0725 10:43:39.185362       1 event.go:307] "Event occurred" object="default/wordpress-mysql-7cc886fc7c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: wordpress-mysql-7cc886fc7c-cp9d6"
I0725 10:43:39.247801       1 event.go:307] "Event occurred" object="default/wordpress-fcfd6d499" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: wordpress-fcfd6d499-27npc"
I0725 10:43:41.926896       1 event.go:307] "Event occurred" object="default/wordpress" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-78bb764d54 to 1"
I0725 10:43:42.164380       1 event.go:307] "Event occurred" object="default/wordpress-78bb764d54" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-78bb764d54-8llzf"
I0725 10:43:53.618570       1 event.go:307] "Event occurred" object="default/wordpress-mysql" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-mysql-b759dbb45 to 1"
I0725 10:43:53.687261       1 event.go:307] "Event occurred" object="default/wordpress-mysql-b759dbb45" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-mysql-b759dbb45-2b5h7"

* 
* ==> kube-controller-manager [acc34c3aa81a] <==
* I0725 10:01:34.751180       1 graph_builder.go:294] "Running" component="GraphBuilder"
I0725 10:01:34.762566       1 controllermanager.go:638] "Started controller" controller="horizontalpodautoscaling"
I0725 10:01:34.762762       1 horizontal.go:200] "Starting HPA controller"
I0725 10:01:34.763335       1 shared_informer.go:311] Waiting for caches to sync for HPA
I0725 10:01:34.801988       1 controllermanager.go:638] "Started controller" controller="attachdetach"
I0725 10:01:34.804482       1 controllermanager.go:638] "Started controller" controller="persistentvolume-expander"
I0725 10:01:34.914052       1 expand_controller.go:339] "Starting expand controller"
I0725 10:01:34.914081       1 shared_informer.go:311] Waiting for caches to sync for expand
I0725 10:01:34.914096       1 attach_detach_controller.go:343] "Starting attach detach controller"
I0725 10:01:34.914102       1 shared_informer.go:311] Waiting for caches to sync for attach detach
I0725 10:01:34.914542       1 controllermanager.go:638] "Started controller" controller="bootstrapsigner"
I0725 10:01:34.914704       1 shared_informer.go:311] Waiting for caches to sync for bootstrap_signer
I0725 10:01:34.964551       1 controllermanager.go:638] "Started controller" controller="pvc-protection"
I0725 10:01:34.964685       1 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0725 10:01:34.964698       1 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0725 10:01:35.012569       1 controllermanager.go:638] "Started controller" controller="pv-protection"
I0725 10:01:35.012863       1 pv_protection_controller.go:78] "Starting PV protection controller"
I0725 10:01:35.012881       1 shared_informer.go:311] Waiting for caches to sync for PV protection
I0725 10:01:35.015457       1 controllermanager.go:638] "Started controller" controller="ttl-after-finished"
I0725 10:01:35.015476       1 ttlafterfinished_controller.go:109] "Starting TTL after finished controller"
I0725 10:01:35.015649       1 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I0725 10:01:35.017513       1 controllermanager.go:638] "Started controller" controller="root-ca-cert-publisher"
I0725 10:01:35.017620       1 publisher.go:101] Starting root CA certificate configmap publisher
I0725 10:01:35.017629       1 shared_informer.go:311] Waiting for caches to sync for crt configmap
I0725 10:01:35.019021       1 controllermanager.go:638] "Started controller" controller="ephemeral-volume"
I0725 10:01:35.019156       1 controller.go:169] "Starting ephemeral volume controller"
I0725 10:01:35.019168       1 shared_informer.go:311] Waiting for caches to sync for ephemeral
I0725 10:01:35.020427       1 controllermanager.go:638] "Started controller" controller="endpoint"
I0725 10:01:35.020572       1 endpoints_controller.go:172] Starting endpoint controller
I0725 10:01:35.020582       1 shared_informer.go:311] Waiting for caches to sync for endpoint
I0725 10:01:35.022955       1 controllermanager.go:638] "Started controller" controller="disruption"
I0725 10:01:35.023080       1 disruption.go:423] Sending events to api server.
I0725 10:01:35.023156       1 disruption.go:434] Starting disruption controller
I0725 10:01:35.023164       1 shared_informer.go:311] Waiting for caches to sync for disruption
I0725 10:01:35.024355       1 controllermanager.go:638] "Started controller" controller="statefulset"
I0725 10:01:35.024532       1 stateful_set.go:161] "Starting stateful set controller"
I0725 10:01:35.024540       1 shared_informer.go:311] Waiting for caches to sync for stateful set
I0725 10:01:35.026349       1 controllermanager.go:638] "Started controller" controller="csrcleaner"
I0725 10:01:35.026472       1 cleaner.go:82] Starting CSR cleaner controller
E0725 10:01:35.094305       1 core.go:92] "Failed to start service controller" err="WARNING: no cloud provider provided, services of type LoadBalancer will fail"
I0725 10:01:35.094324       1 controllermanager.go:616] "Warning: skipping controller" controller="service"
I0725 10:01:35.360005       1 controllermanager.go:638] "Started controller" controller="endpointslicemirroring"
I0725 10:01:35.360213       1 endpointslicemirroring_controller.go:211] Starting EndpointSliceMirroring controller
I0725 10:01:35.361011       1 shared_informer.go:311] Waiting for caches to sync for endpoint_slice_mirroring
I0725 10:01:35.362493       1 controllermanager.go:638] "Started controller" controller="daemonset"
I0725 10:01:35.363663       1 daemon_controller.go:291] "Starting daemon sets controller"
I0725 10:01:35.363745       1 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0725 10:01:35.365025       1 controllermanager.go:638] "Started controller" controller="ttl"
I0725 10:01:35.365850       1 ttl_controller.go:124] "Starting TTL controller"
I0725 10:01:35.365905       1 shared_informer.go:311] Waiting for caches to sync for TTL
I0725 10:01:35.416279       1 controllermanager.go:638] "Started controller" controller="job"
I0725 10:01:35.416492       1 job_controller.go:202] Starting job controller
I0725 10:01:35.417639       1 shared_informer.go:311] Waiting for caches to sync for job
I0725 10:01:35.581166       1 controllermanager.go:638] "Started controller" controller="replicaset"
I0725 10:01:35.581363       1 replica_set.go:201] "Starting controller" name="replicaset"
I0725 10:01:35.581764       1 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0725 10:01:35.583724       1 controllermanager.go:638] "Started controller" controller="tokencleaner"
I0725 10:01:35.583959       1 tokencleaner.go:112] "Starting token cleaner controller"
I0725 10:01:35.584619       1 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0725 10:01:35.585323       1 shared_informer.go:318] Caches are synced for token_cleaner

* 
* ==> kube-proxy [2f0560665690] <==
* E0725 10:02:01.334578       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:02:02.366688       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I0725 10:02:05.143219       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0725 10:02:05.143374       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0725 10:02:05.143446       1 server_others.go:554] "Using iptables proxy"
I0725 10:02:05.387303       1 server_others.go:192] "Using iptables Proxier"
I0725 10:02:05.387400       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0725 10:02:05.387416       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0725 10:02:05.387456       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0725 10:02:05.387520       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0725 10:02:05.388079       1 server.go:658] "Version info" version="v1.27.3"
I0725 10:02:05.388137       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0725 10:02:05.396372       1 config.go:188] "Starting service config controller"
I0725 10:02:05.396443       1 shared_informer.go:311] Waiting for caches to sync for service config
I0725 10:02:05.396473       1 config.go:97] "Starting endpoint slice config controller"
I0725 10:02:05.396506       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0725 10:02:05.396859       1 config.go:315] "Starting node config controller"
I0725 10:02:05.396902       1 shared_informer.go:311] Waiting for caches to sync for node config
I0725 10:02:05.497439       1 shared_informer.go:318] Caches are synced for node config
I0725 10:02:05.499495       1 shared_informer.go:318] Caches are synced for service config
I0725 10:02:05.499506       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0725 10:03:03.200510       1 trace.go:219] Trace[1426498146]: "iptables save" (25-Jul-2023 10:03:00.433) (total time: 2330ms):
Trace[1426498146]: [2.330318228s] [2.330318228s] END

* 
* ==> kube-proxy [86485437ecbd] <==
* E0725 10:01:11.507601       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:12.740458       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:14.966139       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:23.003361       1 node.go:130] Failed to retrieve node info: nodes "minikube" is forbidden: User "system:serviceaccount:kube-system:kube-proxy" cannot get resource "nodes" in API group "" at the cluster scope
I0725 10:01:32.181716       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0725 10:01:32.504096       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0725 10:01:32.504220       1 server_others.go:554] "Using iptables proxy"
I0725 10:01:34.820076       1 server_others.go:192] "Using iptables Proxier"
I0725 10:01:34.820164       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0725 10:01:34.820186       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0725 10:01:34.820242       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0725 10:01:34.906326       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0725 10:01:34.907054       1 server.go:658] "Version info" version="v1.27.3"
I0725 10:01:34.907092       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0725 10:01:34.958396       1 config.go:188] "Starting service config controller"
I0725 10:01:35.014214       1 config.go:97] "Starting endpoint slice config controller"
I0725 10:01:35.014234       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0725 10:01:35.015068       1 config.go:315] "Starting node config controller"
I0725 10:01:35.015218       1 shared_informer.go:311] Waiting for caches to sync for node config
I0725 10:01:35.027627       1 shared_informer.go:311] Waiting for caches to sync for service config
I0725 10:01:35.115262       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0725 10:01:35.128576       1 shared_informer.go:318] Caches are synced for service config
I0725 10:01:35.315736       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [5504c63ee3ce] <==
* E0725 10:00:49.114959       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:49.330052       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:49.330084       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:49.471160       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:49.471191       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:50.228133       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:50.228164       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:50.319027       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:50.319170       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:50.931656       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:50.931688       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:50.933081       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:50.933107       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:51.899862       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:51.899894       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:52.472949       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:52.472984       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:52.611647       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:52.611679       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:53.478850       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:53.478885       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:53.674092       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:53.674139       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:00:54.210277       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:00:54.210309       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:02.266707       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:02.266882       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:03.319927       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:03.320052       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:03.496127       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:03.496163       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:05.350799       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:05.350895       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:05.860304       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:05.860336       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:06.727110       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:06.727242       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:06.952048       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:06.952235       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:07.864963       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:07.865131       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:08.246784       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:08.246816       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:12.888924       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:12.888957       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:12.987826       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:12.987871       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:15.387972       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:15.388031       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:15.396510       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:15.396538       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:16.218232       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0725 10:01:16.218266       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0725 10:01:22.714441       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0725 10:01:22.714475       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
I0725 10:01:37.896689       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0725 10:01:37.896998       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
E0725 10:01:37.897072       1 shared_informer.go:314] unable to sync caches for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0725 10:01:37.897083       1 configmap_cafile_content.go:210] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0725 10:01:37.897140       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [c86bef1f5aa1] <==
* I0725 10:02:05.280168       1 serving.go:348] Generated self-signed cert in-memory
I0725 10:02:05.704650       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0725 10:02:05.704862       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0725 10:02:05.709004       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0725 10:02:05.710524       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0725 10:02:05.711890       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0725 10:02:05.711900       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0725 10:02:05.712104       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0725 10:02:05.712112       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0725 10:02:05.715229       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0725 10:02:05.715328       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0725 10:02:05.812496       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0725 10:02:05.812503       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0725 10:02:05.817336       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0725 10:12:24.089344       1 trace.go:219] Trace[1488442068]: "Scheduling" namespace:logging,name:fluentd-kwbpj (25-Jul-2023 10:12:23.923) (total time: 116ms):
Trace[1488442068]: ---"Computing predicates done" 94ms (10:12:24.039)
Trace[1488442068]: [116.328941ms] [116.328941ms] END
I0725 10:43:42.636324       1 trace.go:219] Trace[1134944097]: "Scheduling" namespace:default,name:wordpress-78bb764d54-8llzf (25-Jul-2023 10:43:42.163) (total time: 459ms):
Trace[1134944097]: ---"Computing predicates done" 459ms (10:43:42.622)
Trace[1134944097]: [459.20836ms] [459.20836ms] END

* 
* ==> kubelet <==
* Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.700383    1709 memory_manager.go:346] "RemoveStaleState removing state" podUID="0f3ca89f-7849-4c02-af1e-75e6fba143e0" containerName="mysql"
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.700388    1709 memory_manager.go:346] "RemoveStaleState removing state" podUID="0f3ca89f-7849-4c02-af1e-75e6fba143e0" containerName="mysql"
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.700397    1709 memory_manager.go:346] "RemoveStaleState removing state" podUID="fdf97e56-8d26-4efc-877b-dc7d241a0d17" containerName="wordpress"
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.700403    1709 memory_manager.go:346] "RemoveStaleState removing state" podUID="0f3ca89f-7849-4c02-af1e-75e6fba143e0" containerName="mysql"
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.722348    1709 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/0f3ca89f-7849-4c02-af1e-75e6fba143e0-kube-api-access-zqcsq" (OuterVolumeSpecName: "kube-api-access-zqcsq") pod "0f3ca89f-7849-4c02-af1e-75e6fba143e0" (UID: "0f3ca89f-7849-4c02-af1e-75e6fba143e0"). InnerVolumeSpecName "kube-api-access-zqcsq". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.735486    1709 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-zqcsq\" (UniqueName: \"kubernetes.io/projected/0f3ca89f-7849-4c02-af1e-75e6fba143e0-kube-api-access-zqcsq\") on node \"minikube\" DevicePath \"\""
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.735507    1709 reconciler_common.go:300] "Volume detached for volume \"pvc-54e1dee5-96cf-47a0-ad68-456083e6e391\" (UniqueName: \"kubernetes.io/host-path/0f3ca89f-7849-4c02-af1e-75e6fba143e0-pvc-54e1dee5-96cf-47a0-ad68-456083e6e391\") on node \"minikube\" DevicePath \"\""
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.762940    1709 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/0f3ca89f-7849-4c02-af1e-75e6fba143e0-fluentd-logs" (OuterVolumeSpecName: "fluentd-logs") pod "0f3ca89f-7849-4c02-af1e-75e6fba143e0" (UID: "0f3ca89f-7849-4c02-af1e-75e6fba143e0"). InnerVolumeSpecName "fluentd-logs". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.837721    1709 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-54e1dee5-96cf-47a0-ad68-456083e6e391\" (UniqueName: \"kubernetes.io/host-path/2589499a-fe60-4dda-966a-0f83ac155528-pvc-54e1dee5-96cf-47a0-ad68-456083e6e391\") pod \"wordpress-mysql-b759dbb45-2b5h7\" (UID: \"2589499a-fe60-4dda-966a-0f83ac155528\") " pod="default/wordpress-mysql-b759dbb45-2b5h7"
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.838077    1709 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vp897\" (UniqueName: \"kubernetes.io/projected/2589499a-fe60-4dda-966a-0f83ac155528-kube-api-access-vp897\") pod \"wordpress-mysql-b759dbb45-2b5h7\" (UID: \"2589499a-fe60-4dda-966a-0f83ac155528\") " pod="default/wordpress-mysql-b759dbb45-2b5h7"
Jul 25 10:43:53 minikube kubelet[1709]: I0725 10:43:53.838210    1709 reconciler_common.go:300] "Volume detached for volume \"fluentd-logs\" (UniqueName: \"kubernetes.io/empty-dir/0f3ca89f-7849-4c02-af1e-75e6fba143e0-fluentd-logs\") on node \"minikube\" DevicePath \"\""
Jul 25 10:43:54 minikube kubelet[1709]: I0725 10:43:54.528339    1709 scope.go:115] "RemoveContainer" containerID="203c4e3dc14c282281a8f836d9657411a431cea836ff2386e65b736a71b1cb94"
Jul 25 10:43:54 minikube kubelet[1709]: I0725 10:43:54.570753    1709 scope.go:115] "RemoveContainer" containerID="70328b72d256c73244173872677389661ede4592365adb2896db03b680063bee"
Jul 25 10:43:54 minikube kubelet[1709]: E0725 10:43:54.571200    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-kwbpj_logging(52bfaed4-3e27-453f-ab54-9c1c39d9df8f)\"" pod="logging/fluentd-kwbpj" podUID=52bfaed4-3e27-453f-ab54-9c1c39d9df8f
Jul 25 10:43:54 minikube kubelet[1709]: I0725 10:43:54.577295    1709 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=0f3ca89f-7849-4c02-af1e-75e6fba143e0 path="/var/lib/kubelet/pods/0f3ca89f-7849-4c02-af1e-75e6fba143e0/volumes"
Jul 25 10:43:55 minikube kubelet[1709]: I0725 10:43:55.111221    1709 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d310cd74f6c87c06c53b69b704df40c23319484f616bec2d83d16b818b41e136"
Jul 25 10:43:55 minikube kubelet[1709]: E0725 10:43:55.136040    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:43:55 minikube kubelet[1709]: E0725 10:43:55.136072    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528
Jul 25 10:43:55 minikube kubelet[1709]: I0725 10:43:55.570585    1709 scope.go:115] "RemoveContainer" containerID="64c2a72c2f249a2741093315993324f17e65852aea806da72505e3b9bc42ba43"
Jul 25 10:43:55 minikube kubelet[1709]: E0725 10:43:55.571269    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-rhj8j_kube-system(21716875-12d0-417b-b3d6-e744765feca0)\"" pod="kube-system/fluentd-rhj8j" podUID=21716875-12d0-417b-b3d6-e744765feca0
Jul 25 10:43:56 minikube kubelet[1709]: E0725 10:43:56.132705    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:43:56 minikube kubelet[1709]: E0725 10:43:56.132740    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528
Jul 25 10:43:57 minikube kubelet[1709]: E0725 10:43:57.138131    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:43:57 minikube kubelet[1709]: E0725 10:43:57.138168    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528
Jul 25 10:44:04 minikube kubelet[1709]: E0725 10:44:04.572963    1709 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:wordpress:6.2.1-apache,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:wordpress,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:WORDPRESS_DB_HOST,Value:wordpress-mysql,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:WORDPRESS_DB_USER,Value:wordpress,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:wordpress-persistent-storage,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-78bb764d54-8llzf_default(0fcdc7e2-700e-4afb-8acc-01356b7f7412): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:04 minikube kubelet[1709]: E0725 10:44:04.572998    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-78bb764d54-8llzf" podUID=0fcdc7e2-700e-4afb-8acc-01356b7f7412
Jul 25 10:44:07 minikube kubelet[1709]: I0725 10:44:07.569939    1709 scope.go:115] "RemoveContainer" containerID="70328b72d256c73244173872677389661ede4592365adb2896db03b680063bee"
Jul 25 10:44:07 minikube kubelet[1709]: E0725 10:44:07.570080    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-kwbpj_logging(52bfaed4-3e27-453f-ab54-9c1c39d9df8f)\"" pod="logging/fluentd-kwbpj" podUID=52bfaed4-3e27-453f-ab54-9c1c39d9df8f
Jul 25 10:44:07 minikube kubelet[1709]: I0725 10:44:07.570678    1709 scope.go:115] "RemoveContainer" containerID="64c2a72c2f249a2741093315993324f17e65852aea806da72505e3b9bc42ba43"
Jul 25 10:44:07 minikube kubelet[1709]: E0725 10:44:07.570959    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-rhj8j_kube-system(21716875-12d0-417b-b3d6-e744765feca0)\"" pod="kube-system/fluentd-rhj8j" podUID=21716875-12d0-417b-b3d6-e744765feca0
Jul 25 10:44:07 minikube kubelet[1709]: E0725 10:44:07.572018    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:07 minikube kubelet[1709]: E0725 10:44:07.572041    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528
Jul 25 10:44:19 minikube kubelet[1709]: E0725 10:44:19.571660    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:19 minikube kubelet[1709]: E0725 10:44:19.572156    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528
Jul 25 10:44:19 minikube kubelet[1709]: E0725 10:44:19.571847    1709 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:wordpress:6.2.1-apache,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:wordpress,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:WORDPRESS_DB_HOST,Value:wordpress-mysql,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:WORDPRESS_DB_USER,Value:wordpress,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:wordpress-persistent-storage,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-78bb764d54-8llzf_default(0fcdc7e2-700e-4afb-8acc-01356b7f7412): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:19 minikube kubelet[1709]: E0725 10:44:19.573451    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-78bb764d54-8llzf" podUID=0fcdc7e2-700e-4afb-8acc-01356b7f7412
Jul 25 10:44:20 minikube kubelet[1709]: I0725 10:44:20.571148    1709 scope.go:115] "RemoveContainer" containerID="70328b72d256c73244173872677389661ede4592365adb2896db03b680063bee"
Jul 25 10:44:21 minikube kubelet[1709]: I0725 10:44:21.569764    1709 scope.go:115] "RemoveContainer" containerID="64c2a72c2f249a2741093315993324f17e65852aea806da72505e3b9bc42ba43"
Jul 25 10:44:21 minikube kubelet[1709]: E0725 10:44:21.569990    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-rhj8j_kube-system(21716875-12d0-417b-b3d6-e744765feca0)\"" pod="kube-system/fluentd-rhj8j" podUID=21716875-12d0-417b-b3d6-e744765feca0
Jul 25 10:44:25 minikube kubelet[1709]: I0725 10:44:25.368653    1709 scope.go:115] "RemoveContainer" containerID="70328b72d256c73244173872677389661ede4592365adb2896db03b680063bee"
Jul 25 10:44:25 minikube kubelet[1709]: I0725 10:44:25.368925    1709 scope.go:115] "RemoveContainer" containerID="66749532e8f1a5bf18cb9e1e1966d7b51d808d5b1b59ed3f647469bdec547e48"
Jul 25 10:44:25 minikube kubelet[1709]: E0725 10:44:25.369066    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-kwbpj_logging(52bfaed4-3e27-453f-ab54-9c1c39d9df8f)\"" pod="logging/fluentd-kwbpj" podUID=52bfaed4-3e27-453f-ab54-9c1c39d9df8f
Jul 25 10:44:31 minikube kubelet[1709]: E0725 10:44:31.571327    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:31 minikube kubelet[1709]: E0725 10:44:31.571852    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528
Jul 25 10:44:32 minikube kubelet[1709]: E0725 10:44:32.576014    1709 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:wordpress:6.2.1-apache,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:wordpress,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:WORDPRESS_DB_HOST,Value:wordpress-mysql,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:WORDPRESS_DB_USER,Value:wordpress,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:wordpress-persistent-storage,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-78bb764d54-8llzf_default(0fcdc7e2-700e-4afb-8acc-01356b7f7412): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:32 minikube kubelet[1709]: E0725 10:44:32.576052    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-78bb764d54-8llzf" podUID=0fcdc7e2-700e-4afb-8acc-01356b7f7412
Jul 25 10:44:34 minikube kubelet[1709]: I0725 10:44:34.570154    1709 scope.go:115] "RemoveContainer" containerID="64c2a72c2f249a2741093315993324f17e65852aea806da72505e3b9bc42ba43"
Jul 25 10:44:34 minikube kubelet[1709]: E0725 10:44:34.570737    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-rhj8j_kube-system(21716875-12d0-417b-b3d6-e744765feca0)\"" pod="kube-system/fluentd-rhj8j" podUID=21716875-12d0-417b-b3d6-e744765feca0
Jul 25 10:44:37 minikube kubelet[1709]: I0725 10:44:37.570492    1709 scope.go:115] "RemoveContainer" containerID="66749532e8f1a5bf18cb9e1e1966d7b51d808d5b1b59ed3f647469bdec547e48"
Jul 25 10:44:37 minikube kubelet[1709]: E0725 10:44:37.570777    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-kwbpj_logging(52bfaed4-3e27-453f-ab54-9c1c39d9df8f)\"" pod="logging/fluentd-kwbpj" podUID=52bfaed4-3e27-453f-ab54-9c1c39d9df8f
Jul 25 10:44:42 minikube kubelet[1709]: E0725 10:44:42.574706    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:42 minikube kubelet[1709]: E0725 10:44:42.575363    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528
Jul 25 10:44:43 minikube kubelet[1709]: E0725 10:44:43.572402    1709 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:wordpress:6.2.1-apache,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:wordpress,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:WORDPRESS_DB_HOST,Value:wordpress-mysql,ValueFrom:nil,},EnvVar{Name:WORDPRESS_DB_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:WORDPRESS_DB_USER,Value:wordpress,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:wordpress-persistent-storage,ReadOnly:false,MountPath:/var/www/html,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-78bb764d54-8llzf_default(0fcdc7e2-700e-4afb-8acc-01356b7f7412): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:43 minikube kubelet[1709]: E0725 10:44:43.572439    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-78bb764d54-8llzf" podUID=0fcdc7e2-700e-4afb-8acc-01356b7f7412
Jul 25 10:44:46 minikube kubelet[1709]: I0725 10:44:46.571332    1709 scope.go:115] "RemoveContainer" containerID="64c2a72c2f249a2741093315993324f17e65852aea806da72505e3b9bc42ba43"
Jul 25 10:44:46 minikube kubelet[1709]: E0725 10:44:46.571962    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-rhj8j_kube-system(21716875-12d0-417b-b3d6-e744765feca0)\"" pod="kube-system/fluentd-rhj8j" podUID=21716875-12d0-417b-b3d6-e744765feca0
Jul 25 10:44:51 minikube kubelet[1709]: I0725 10:44:51.569842    1709 scope.go:115] "RemoveContainer" containerID="66749532e8f1a5bf18cb9e1e1966d7b51d808d5b1b59ed3f647469bdec547e48"
Jul 25 10:44:51 minikube kubelet[1709]: E0725 10:44:51.570025    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fluentd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=fluentd pod=fluentd-kwbpj_logging(52bfaed4-3e27-453f-ab54-9c1c39d9df8f)\"" pod="logging/fluentd-kwbpj" podUID=52bfaed4-3e27-453f-ab54-9c1c39d9df8f
Jul 25 10:44:53 minikube kubelet[1709]: E0725 10:44:53.572460    1709 kuberuntime_manager.go:1212] container &Container{Name:mysql,Image:mysql:8.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:mysql,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},EnvVar{Name:MYSQL_DATABASE,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_USER,Value:wordpress,ValueFrom:nil,},EnvVar{Name:MYSQL_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mysql-pass,},Key:password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:mysql-persistent-storage,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-vp897,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-mysql-b759dbb45-2b5h7_default(2589499a-fe60-4dda-966a-0f83ac155528): CreateContainerConfigError: secret "mysql-pass" not found
Jul 25 10:44:53 minikube kubelet[1709]: E0725 10:44:53.572494    1709 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"secret \\\"mysql-pass\\\" not found\"" pod="default/wordpress-mysql-b759dbb45-2b5h7" podUID=2589499a-fe60-4dda-966a-0f83ac155528

* 
* ==> storage-provisioner [17f784edca0b] <==
* I0725 10:03:33.634652       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0725 10:03:34.377224       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0725 10:03:34.401021       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0725 10:03:53.569597       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0725 10:03:53.592142       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b0177226-c3d6-46b9-9d19-a4f1d4e824d7!
I0725 10:03:53.592093       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"56fa6f4e-b777-466e-8061-92a420e63a21", APIVersion:"v1", ResourceVersion:"51601", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b0177226-c3d6-46b9-9d19-a4f1d4e824d7 became leader
I0725 10:03:54.130203       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b0177226-c3d6-46b9-9d19-a4f1d4e824d7!

* 
* ==> storage-provisioner [9d2f13e96235] <==
* I0725 10:02:01.651924       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0725 10:02:01.661315       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

